# Introduction to Integration

In this section, we examine the other half of calculus, integral calculus.
To begin, we introduce some new notation. First, suppose we have the
function ${y = f(x).}$ The differential of ${y}$ is denoted ${dy,}$ where:

$$
	dy = f'(x)dx
$$

Because ${y}$ is equal to ${f(x),}$ we often read ${dy}$ as the
differential of ${f.}$ Notice that this notation looks similar to the
notation for a derivative:

$$
	\begin{aligned} dy &= f'(x)dx \\[1em] \dfrac{dy}{dx} &= f'(x) \end{aligned}
$$

Indeed, the notations are closely related:

$$
	dy = f'(x)dx \iff \dfrac{dy}{dx} = f'(x)
$$

This close relationship is no accident. It stems directly from the
Leibnizian (after the mathematician Gottfried Wilhelm Leibniz)
interpretation of the derivative: Derivatives are ratios of differentials.
Hence the notation:

$$
	\dfrac{dy}{dx} = f'(x)
$$

So ${dy}$ and ${dx}$ are differentials. But what is a differential? A
differential is a kind of _infinitesimal_ â€” an infinitely small quantity. Thus,
whenever we see the notation:

$$
	\large dx
$$

or anything that looks like it (e.g., ${dy,}$ ${d\theta}$, ${dt,}$ etc.), we
want to think: "A very, very tiny bit of ${x}$" (or, a very, very tiny bit of
${y,}$ ${\theta,}$ ${t,}$ etc.)

Leibniz is credited with perfecting techniques for handling infinitesimals.
In part because of how effective his notation was &mdash; it's far, far more
effective than Newton's.[^notation_note] In fact, so much so that some
historians argue that the reliance on Newton's notation set British mathematics
behind continental Europe's by over a century.

[^notation_note]:
    Constructing a notation system is an exercise in abstraction. A good
    notation system can significantly impact how easy or difficult a
    problem is &mdash; it allows its user to rapidly draw inferences, which
    is precisely how mathematics is done. It doesn't take much imagination
    to see why this is the case. If we had to write computer programs in
    binary &mdash; the language understood by computers &mdash; it's
    unlikely we'd see the myriad of technologies we see today.

A good way to see how the notation works is through linear approximation.
Recall how we used ${\Delta x}$ and ${\Delta y}$ to denote the change in
${x}$ and the change in ${y}$ respectively:

<Fig
	link={
		"https://res.cloudinary.com/sublimis/image/upload/v1657503523/math/newton_notation_infinitesimal_k83opy.svg"
	}
	imwidth={"300"}
	imheight={"260"}
	caption={"Newton's notation"}
	width={"60"}
/>

This is Newton's notation. With Leibniz notation, we replace ${\Delta x}$
with ${dx,}$ and ${\Delta y}$ with ${dy:}$

<Fig
	link={
		"https://res.cloudinary.com/sublimis/image/upload/v1657503562/math/leibniz_notation_infinitesimal_j3hups.svg"
	}
	imwidth={"300"}
	imheight={"260"}
	caption={"Leibniz's notation"}
	width={"60"}
/>

For example, consider the problem:

> **problem** What is ${(64.1)^{\frac{1}{3}}}$ approximately equal to?

We start by writing:

$$
	y = x^{\frac{1}{3}}
$$

With Leibniz notation, we take the differential of ${y:}$

$$
	dy = \dfrac{1}{3}x^{-\frac{2}{3}} dx
$$

Following the same ideas we saw with linear approximations, we pick a
starting point, say ${x = 64:}$

$$
	y = 64^{\frac{1}{3}} = 4
$$

Returning to Leibniz notation, we get:

$$
	\begin{aligned} dy &= \dfrac{1}{3} (64)^{- \frac{2}{3}}dx \\[1em] &= \dfrac{1}{3} \dfrac{1}{16} dx \\[1em] &= \dfrac{1}{48} dx \end{aligned}
$$

Given ${x = 64,}$ our equation is:

$$
	x + dx = 64.1
$$

This means that:

$$
	\begin{aligned} x + dx &= 64.1 \\ 64 + dx &= 64.1 \\ dx &= 0.1 \end{aligned}
$$

Hence, we know that ${dx = 1/10.}$ This is the increment, or infinitesimal
change, we're interested in. Carrying out the approximation. We know that
${dy = \dfrac{1}{48}dx,}$ so:

$$
	\begin{aligned} (64.1)^{\frac{1}{3}} &\approx y + dy \\[1em] &\approx 4 + \dfrac{1}{48}dx \\[1em] &\approx 4 + \dfrac{1}{48} \cdot \dfrac{1}{10} \\[1em] &\approx 4 + \dfrac{1}{480} \\[1em] &\approx 4.002 \end{aligned}
$$

## Antiderivatives

Another piece of notation we'll introduce is the following:

$$
	\textcolor{blue}{G(x)} = \textcolor{tomato}{\int} g(x) dx
$$

In the notation above, ${\textcolor{blue}{G(x)}}$ is called the
**antiderivative** or **indefinite integral** of ${g(x).}$ The
${\textcolor{tomato}{\int}}$ is the **integral symbol**. To understand
what these terms mean, let's start with some examples. When we see the
expression:

$$
	\int \sin x~dx
$$

we read it as "the integral of ${\sin x.}$" The integral of ${\sin x}$ is a
function whose derivative is ${\sin x.}$ From our discussion of
derivatives, we know that the derivative of ${- \cos x}$ is ${\sin x.}$
Accordingly, we say that ${- \cos x}$ is the antiderivative, or indefinite
integral, of ${\sin x:}$

$$
	G(x) = \int \sin x~dx = - \cos x
$$

But why is it called "indefinite?"" Because the derivative of a constant is
zero, so we can add any constant to ${- \cos x}$ and still get the
derivative ${\sin x:}$

$$
	G(x) = \int \sin x~dx = - \cos x + C
$$

Whenever we take the antiderivative of a function, it's ambiguous up to
some constant. Let's consider another example. What does this expression
evaluate to:

$$
	\int x^a~dx
$$

This example is essentially asking us, "What function, when differentiated,
yields ${x^a?}$" We know from the power rule that:

$$
	(x^a)' = ax^{a-1}
$$

Thus, to get ${x^a,}$ we need to cancel out the ${a:}$

$$
	\int x^a~dx = \dfrac{1}{a+1}x^{a+1}
$$

Checking our work using differential notation:

$$
	\begin{aligned} d(x^{a+1}) &= (a+1)x^a~dx \\ (a+1)x^{a+1-1} &= (a+1)x^a~dx \\ \cancel{(a+1)}x^{a} &= \cancel{(a+1)}x^a~dx \\ \end{aligned}
$$

And because we can add any constant, we must write:

$$
	\int x^a~dx = \dfrac{1}{a+1}x^{a+1} + C
$$

Furthermore, examining the antiderivative, we see a restriction. The
proposition:

$$
	\int x^a dx = \dfrac{1}{a+1} x^{a+1} + C
$$

is true if, and only if, ${a \neq -1.}$ On the other hand, the proposition:

$$
	d(x^{a+1}) = (a+1)x^a~dx
$$

contains no such restriction. Consider another problem. Evaluate the
expression below:

$$
	\int \dfrac{dx}{x}
$$

The first step is to rewrite the expression into a more familiar form:

$$
	\int \dfrac{1}{x}~dx
$$

Looking at this, we immediately see that the function we're looking for is
${\ln x}$ (don't forget the constant):

$$
	\int \dfrac{1}{x}~dx = \ln x + C
$$

If we think carefully about the derivative of ${f(x) = \ln x,}$ we'd
realize that we can also get the derivative ${f'(x) = \dfrac{1}{x}}$ when
${x}$ is negative:

$$
	\begin{aligned} \dfrac{d}{dx} \ln (-x) &= \dfrac{1}{-x} \cdot \dfrac{d}{dx} (-x) \\[1em] &= \dfrac{1}{-x} \cdot -1 \\[1em] &= \dfrac{-1}{-x} \\[1em] &= \dfrac{1}{x} \end{aligned}
$$

Accordingly, the antiderivative is more properly written as:

$$
	\int \dfrac{1}{x}~dx = \ln\lvert x \rvert + C
$$

Another example:

$$
	\int \sec^2x~dx
$$

Here we get:

$$
	\int \sec^2x~dx = \tan x + C
$$

The addition of a constant is the only ambiguous thing about
antiderivatives. If we examine the antiderivative closely, we can draw a
few inferences. Say we had some function ${F(x)}$ and another function
${G(x)}$ If ${F'(x) = G'(x),}$ then it follows that:

$$
	\begin{aligned} (F(x)-G(x))' &= F'(x) - G'(x) \\ &= 0 \end{aligned}
$$

If ${(F(x)-G(x))' = 0,}$ then it must be the case that:

$$
	F(x) - G(x) = C
$$

Rearranging:

$$
	F(x) = G(x) + C
$$

In other words, ${F(x) - G(x)}$ results in a constant.[^note_notice]

[^note_notice]:
    Notice that this was one of the lemmas we drew from the mean value
    theorem.

What this tells us is that the addition of a constant is the only ambiguous
thing about antiderivatives. Beyond that constant, the antiderivative is
unique &mdash; the antiderivative ${F(x),}$ where ${F(x) = G(x) + C,}$ is
the only antiderivative defined as ${G(x) + C.}$

> **antiderivative uniqueness theorem** If ${F'(x) = G'(x),}$ then
> ${F(x) = G(x) + C.}$
