<Head>
	<title>Integration</title>
	<meta name={`description`} content={`Notes on integration.`}/>
</Head>

# Integration

_This note presents an overview of integration_.

## Antiderivatives
> __~definition~.__ Let ${f}$ be a function in ${\reals,}$ and let ${\cic{a}{b}}$ be a closed interval in ${\reals.}$ If there exists a function ${F}$ such that (1) ${F}$ is continuous on ${\cic{a}{b}}$ and (2) ${F'(x)=f(x)}$ for all ${x \in \oio{a}{b},}$ then we say that ${f}$ has the _antiderivative_ ${G,}$ unique to the addition of a constant.


## Notation
Suppose we have the function ${y = f(x).}$ The differential of ${y}$ is denoted ${\d y,}$ where ${\d y = f'(x)\d x}$ Because ${y}$ is equal to ${f(x),}$ we often read ${dy}$ as the differential of ${f.}$ The notations are closely related:

$$
	\d y = f'(x)\d x \iff \di{y}{x} = f'(x).
$$

This close relationship is no accident. It stems directly from the Leibnizian (after the mathematician Gottfried Wilhelm Leibniz) interpretation of the derivative: Derivatives are ratios of differentials. Leibniz is credited with perfecting techniques for handling infinitesimals. In part because of how effective his notation was &mdash; it's far, far more effective than Newton's.[^notation_note] In fact, so much so that some historians argue that the reliance on Newton's notation set British mathematics behind continental Europe's by over a century. A good way to see how the notation works is through linear approximation. Recall how we used ${\Delta x}$ and ${\Delta y}$ to denote the change in ${x}$ and the change in ${y}$ respectively.

[^notation_note]:
    Constructing a notation system is an exercise in abstraction. A good notation system can significantly impact how easy or difficult a problem is &mdash; it allows its user to rapidly draw inferences, which is precisely how mathematics is done. It doesn't take much imagination to see why this is the case. If we had to write computer programs in binary &mdash; the language understood by computers &mdash; it's unlikely we'd see the myriad of technologies we see today.

<Grid cols={2}>
<Fig
	link={
		"https://res.cloudinary.com/sublimis/image/upload/v1657503523/math/newton_notation_infinitesimal_k83opy.svg"
	}
	imwidth={"300"}
	imheight={"260"}
	caption={"Newton's notation"}
	width={"100"}
/>
<Fig
	link={
		"https://res.cloudinary.com/sublimis/image/upload/v1657503562/math/leibniz_notation_infinitesimal_j3hups.svg"
	}
	imwidth={"300"}
	imheight={"260"}
	caption={"Leibniz's notation"}
	width={"100"}
/>
</Grid>

~example~. What is ${(64.1)^{\frac{1}{3}}}$ approximately equal to? We start by writing ${y = x^{1/3}.}$ With Leibniz notation, we take the differential of ${y:}$ ${\d y = \frac{1}{3}x^{-\frac{2}{3}} \d x.}$ Following the same ideas we saw with linear approximations, we pick a starting point, say ${x = 64.}$ Now we have ${y = 64^{\frac{1}{3}} = 4.}$ Returning to Leibniz notation, we get:

$$
	\begin{aligned} \d y &= \dfrac{1}{3} (64)^{- \frac{2}{3}}\d x \\[1em] &= \dfrac{1}{3} \dfrac{1}{16} \d x \\[1em] &= \dfrac{1}{48} \d x \end{aligned}
$$

Given ${x = 64,}$ our equation is therefore ${x + dx = 64.1}$
This implies that:

$$
	\begin{aligned} x + \d x &= 64.1 \\ 64 + \d x &= 64.1 \\ \d x &= 0.1 \end{aligned}
$$

Hence, we know that ${\d x = 1/10.}$ This is the increment, or infinitesimal change, we're interested in. Carrying out the approximation. We know that ${\d y = \frac{1}{48}\d x,}$ so:

$$
	\begin{aligned} (64.1)^{\frac{1}{3}} &\approx y + \d y \\[1em] &\approx 4 + \dfrac{1}{48}\d x \\[1em] &\approx 4 + \dfrac{1}{48} \cdot \dfrac{1}{10} \\[1em] &\approx 4 + \dfrac{1}{480} \\[1em] &\approx 4.002 \end{aligned}
$$

## Antiderivatives
> __~definition~.__ Let ${f:D\subseteq \reals \to \reals.}$ If there exists a function ${G}$ continuous on ${\cic{a}{b}}$ such that ${G'(x)=f(x)}$ for all ${x \in \oio{a}{b},}$ then we say that ${f}$ has an antideriative ${G}$ on ${\cic{a}{b}.}$ If a function ${f}$ has an antiderivative ${G,}$ then ${G}$ is unique up to the addition of a constant.

~example~. Below is a new bit of notation.

$$
	\textcolor{teal}{G(x)} = \textcolor{firebrick}{\int} g(x) dx
$$

In the notation above, ${\textcolor{teal}{G(x)}}$ is called the __antiderivative__ or __indefinite integral__ of ${g(x).}$ The ${\textcolor{tomato}{\int}}$ is the __integral symbol__. To understand what these terms mean, let's start with some examples. When we see the expression:

$$
	\int \sin x~dx
$$

we read it as "the integral of ${\sin x.}$" The integral of ${\sin x}$ is a function whose derivative is ${\sin x.}$ From our discussion of derivatives, we know that the derivative of ${- \cos x}$ is ${\sin x.}$ Accordingly, we say that ${- \cos x}$ is the antiderivative, or indefinite integral, of ${\sin x:}$

$$
	G(x) = \int \sin x~dx = - \cos x
$$

But why is it called "indefinite?"" Because the derivative of a constant is zero, so we can add any constant to ${- \cos x}$ and still get the derivative ${\sin x:}$

$$
	G(x) = \int \sin x~dx = - \cos x + C
$$

Whenever we take the antiderivative of a function, it's ambiguous up to some constant. Let's consider another example. What does this expression evaluate to:
$$
	\int x^a~dx
$$

This example is essentially asking us, "What function, when differentiated, yields ${x^a?}$" We know from the power rule that:

$$
	(x^a)' = ax^{a-1}
$$

Thus, to get ${x^a,}$ we need to cancel out the ${a:}$

$$
	\int x^a~dx = \dfrac{1}{a+1}x^{a+1}
$$

Checking our work using differential notation:

$$
	\begin{aligned} d(x^{a+1}) &= (a+1)x^a~dx \\ (a+1)x^{a+1-1} &= (a+1)x^a~dx \\ \cancel{(a+1)}x^{a} &= \cancel{(a+1)}x^a~dx \\ \end{aligned}
$$

And because we can add any constant, we must write:

$$
	\int x^a~dx = \dfrac{1}{a+1}x^{a+1} + C
$$

Furthermore, examining the antiderivative, we see a restriction. The
proposition:

$$
	\int x^a dx = \dfrac{1}{a+1} x^{a+1} + C
$$

is true if, and only if, ${a \neq -1.}$ On the other hand, the proposition:

$$
	d(x^{a+1}) = (a+1)x^a~dx
$$

contains no such restriction. Consider another problem. Evaluate the
expression below:

$$
	\int \dfrac{dx}{x}
$$

The first step is to rewrite the expression into a more familiar form:

$$
	\int \dfrac{1}{x}~dx
$$

Looking at this, we immediately see that the function we're looking for is
${\ln x}$ (don't forget the constant):

$$
	\int \dfrac{1}{x}~dx = \ln x + C
$$

If we think carefully about the derivative of ${f(x) = \ln x,}$ we'd
realize that we can also get the derivative ${f'(x) = \dfrac{1}{x}}$ when
${x}$ is negative:

$$
	\begin{aligned} \dfrac{d}{dx} \ln (-x) &= \dfrac{1}{-x} \cdot \dfrac{d}{dx} (-x) \\[1em] &= \dfrac{1}{-x} \cdot -1 \\[1em] &= \dfrac{-1}{-x} \\[1em] &= \dfrac{1}{x} \end{aligned}
$$

Accordingly, the antiderivative is more properly written as:

$$
	\int \dfrac{1}{x}~dx = \ln\lvert x \rvert + C
$$

Another example:

$$
	\int \sec^2x~dx
$$

Here we get:

$$
	\int \sec^2x~dx = \tan x + C
$$

The addition of a constant is the only ambiguous thing about
antiderivatives. If we examine the antiderivative closely, we can draw a
few inferences. Say we had some function ${F(x)}$ and another function
${G(x)}$ If ${F'(x) = G'(x),}$ then it follows that:

$$
	\begin{aligned} (F(x)-G(x))' &= F'(x) - G'(x) \\ &= 0 \end{aligned}
$$

If ${(F(x)-G(x))' = 0,}$ then it must be the case that:

$$
	F(x) - G(x) = C
$$

Rearranging:

$$
	F(x) = G(x) + C
$$

In other words, ${F(x) - G(x)}$ results in a constant.[^note_notice]

[^note_notice]:
    Notice that this was one of the lemmas we drew from the mean value
    theorem.

What this tells us is that the addition of a constant is the only ambiguous
thing about antiderivatives. Beyond that constant, the antiderivative is
unique &mdash; the antiderivative ${F(x),}$ where ${F(x) = G(x) + C,}$ is
the only antiderivative defined as ${G(x) + C.}$

> __antiderivative uniqueness theorem__ If ${F'(x) = G'(x),}$ then
> ${F(x) = G(x) + C.}$
