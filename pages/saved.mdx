
_todo: entry point time_.

Suppose we're given a sequence of slots, each occupied by some object.

$$
	\underset{1}{\sd{\no{\circ}}}
	\underset{2}{\sd{\no{\circ}}}
	\underset{3}{\sd{\no{\circ}}}
	\underset{4}{\sd{\no{\circ}}}
	\underset{5}{\sd{\no{\circ}}}
$$

There are several ways to fill and empty the sequence. Below, each row of squares corresponds to a state of the sequence. In method 1, the sequence is filled from right to left, and emptied from right to left. In method 2, the sequence is filled from right to left, and emptied from left to right. In method 3, the sequence is filled and emptied from both ends, with the last inserted element exiting first, and the first inserted element exiting last. In method 4, the sequence is filled and emptited from both ends, with the first inserted element exiting first, and the last inserted element exiting last. In method 5, the sequence is filled from both ends, but can only be emptied from one end. In method 6, the sequence is emptied from both ends, but can only be filled from one end.

<Grid cols={3}>
$$
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	\underset{1}{\sd{\no{\circ}}}\underset{2}{\sd{\no{\circ}}}\underset{3}{\sd{\no{\circ}}}\underset{4}{\sd{\no{\circ}}}\underset{5}{\sd{\no{\circ}}}
$$
$$
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}} \\
	\underset{1}{\sd{\no{\circ}}}\underset{2}{\sd{\no{\circ}}}\underset{3}{\sd{\no{\circ}}}\underset{4}{\sd{\no{\circ}}}\underset{5}{\sd{\no{\circ}}}
$$
$$
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	\underset{1}{\sd{\no{\circ}}}\underset{2}{\sd{\no{\circ}}}\underset{3}{\sd{\no{\circ}}}\underset{4}{\sd{\no{\circ}}}\underset{5}{\sd{\no{\circ}}}
$$
$$
	\tx{method 1}
$$
$$
	\tx{method 2}
$$
$$
	\tx{method 3}
$$
$$
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}} \\
	\underset{1}{\sd{\no{\circ}}}\underset{2}{\sd{\no{\circ}}}\underset{3}{\sd{\no{\circ}}}\underset{4}{\sd{\no{\circ}}}\underset{5}{\sd{\no{\circ}}}
$$
$$
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	\underset{1}{\sd{\no{\circ}}}\underset{2}{\sd{\no{\circ}}}\underset{3}{\sd{\no{\circ}}}\underset{4}{\sd{\no{\circ}}}\underset{5}{\sd{\no{\circ}}}
$$
$$
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}} \\
	{\sd{\no{\circ}}}{\sd{\no{\circ}}}{\bd{\no{\circ}}}{\sd{\no{\circ}}}{\sd{\no{\circ}}} \\
	\underset{1}{\sd{\no{\circ}}}\underset{2}{\sd{\no{\circ}}}\underset{3}{\sd{\no{\circ}}}\underset{4}{\sd{\no{\circ}}}\underset{5}{\sd{\no{\circ}}}
$$
$$
	\tx{method 4}
$$
$$
	\tx{method 5}
$$
$$
	\tx{method 6}
$$
</Grid>

Each of the methods corresponds to _fundamental sequence data types_. Method 1 corresponds to a _stack_, method 2 corresponds to a _queue_, method 3 corresponds to a _double stack_, method 4 corresponds to a _deque_, method 5 corresponds to a _shelf_, and method 6 corresponds to a _scroll_.


## Operating System
While the CPU is certainly one of the most important chips on a computer system, it's not necessarily the "highest-ranking" or even the "brain" of a computer system. Because of concurrency, the modern computer system is highly-modularized, resulting in some chips on a computer system working largely independent of the CPU (e.g., the graphics adapter, sound adapter, and netork adapter). Modularity is what ensures that a computer system can still work (albeit in a very limited fashion) should the CPU somehow fail momentarily. This does, however, come at a cost — all of these different chips are competing for RAM. That said, if these other chips want to send data to another chip (which underlies a significant amount of our interactions with a computer system), they will have to rely on the CPU.

$$
	\tx{RAM} \iff \case{
		\iff \tx{CPU} \\
		\iff \tx{disk controller} \iff \df{ssd/hdd} \\
		\iff \tx{graphics adapter} \iff \tx{monitor} \\
		\iff \tx{sound adapter} \iff \tx{speaker} \\
		\iff \tx{network adapter} \case{
			\iff \tx{ethernet} \\
			\iff \tx{wifi antenna} \\
			\iff \tx{bluetooth antenna}
		} \\
		\iff \tx{usb controller} \case{
			\iff \tx{mouse/trackpad} \\
			\iff \tx{keyboard}
		} \\
	}
$$

> __~device-driver~.__ A program ${(H,A \mapsto O)}$ where ${H}$ is a set of hardware, ${A}$ is a set of parameters called the _API_, and ${O}$ is a set of operations on ${H.}$

### Bootstrap Program
The _bootstrap program_ is the program that runs when a computer system is powered on or rebooted. This program is stored in ROM. The program's two primary tasks are: (1) Loading the operating system into memory and commence its execution, (2) locate 

## Processes and Threads
A program, on its own, is just a set of instructions. Below, each block is an instruction.

$$
	\small{\ax{
		\sd{1\no{~~~~~~~~}} \\
		\sd{2\no{~~~~~~~~}} \\
		\sd{3\no{~~~~~~~~}} \\
		\sd{4\no{~~~~~~~~}} \\
	}}
$$

To get this program to do something we must send each instruction to the CPU.


$$
	\sd{4\no{~~~~~~~~}}
	\sd{3\no{~~~~~~~~}}
	\sd{2\no{~~~~~~~~}}
	\sd{1\no{~~~~~~~~}} \to \df{cpu}
$$

The sequence of instructions sent is called a _thread_. In the old days, programs could only have one thread. Today, however, we can have _multiple threads_. Multiple threads are a response to the fact that modern programs do many different things, and each of those different things consists of potentially many different instructions. Playing a song on a music-streaming application may require opening a socket and sending an HTTP request. Liking a song may require opening another socket and forwarding data towards some server possibly hundreds of miles away. Such operations are undoubtedly complex, and require multiple instructions.


$$
	\small{\ax{
		\sd{~\tx{play music}~} \\
		\sd{~ ~ ~\tx{like song}~ ~ ~}
	}}
$$

Rather than sending these instructions on just one path to the CPU — a single thread — modern computers allow us to send them on multiple paths. We call this _multithreading_.


$$
	\rcase{\sd{~\tx{play music}~} \\
	\sd{~ ~ ~\tx{like song}~ ~ ~}} \to \df{cpu}
$$

For a computer system with just one CPU, the system _cannot_ execute these sets of instructions all at once. So, the CPU designers must be a bit more clever and use _time slicing_: Switch processing each thread rapidly enough, and it _looks like_ the threads are processed simultaneously.

$$
	\footnotesize\sd{{\tx{play music}}}\no{\sd{\tx{like song}}}\sd{{\tx{play music}}}\no{\sd{\tx{like song}}}\sd{{\tx{play music}}}\no{\sd{\tx{like song}}}\sd{{\tx{play music}}}\no{\sd{\tx{like song}}} \to \tx{thread 1} \\
	\no{\sd{\tx{play music}}}\wd{\tx{like song}}\no{\sd{\tx{play music}}}\wd{\tx{like song}}\no{\sd{\tx{play music}}}\wd{\tx{like song}}\no{\sd{\tx{play music}}}\wd{\tx{like song}} \to \tx{thread 2}
$$

Every program instruction requires some _resource_, whether that's allocated
memory for the program's use or processing power to transfer data to and from
memory. To keep track of all these demands, the CPU maintains a record of every
thread's _state_ when it jumps from one thread to the next: What was the last
instruction executed, what was the last incoming request, what instruction
should be executed upon returning, and so on. The set of all the thread states
is called a _context_, and the set of all threads and their context is called a
_process_. All of this works at break-neck velocities. Switching between ~firefox~ and ~word~ is seamless, disguising the fact that in less than a blink of an eye, the CPU didn't just jump between threads, it jumped between entire processes — ~firefox~ and ~word~ are two separate programs.


## Polar Coordinates
> __~circle~.__ We define a circle as the relation ${r = \sqrt{x^2 + y^2},}$ where ${r}$ a number called the _radius_ of the circle, and ${x}$ and ${y}$ are numbers.

> __~cartesian plane~.__ We define the Cartesian plane as the set of all pairs ${\reals \times \reals,}$ where each ${(x,y) \in \reals}$ is called a _Cartesian coordinate_.

> __~polar coordinates~.__ Given the Cartesian coordinate ${(x,y),}$ we define the pair ${(r \cos \theta, r \sin \theta)}$ where ${r}$ is the radius of a circle, ${\theta}$ is a number, such that ${(x,y) = (r \cos \theta, r \sin \theta).}$

## Physical Vector
> __~definition~.__ A _physical vector_ is a triple ${\vc{v} = (m,v_x,v_y)}$ where ${v_x}$ and ${v_y}$ form a pair ${(v_x=v \cos \theta, v_y=v \sin \theta)}$ called the _direction_ of ${\vc{v},}$ and ${m = \sqrt{(v \cos \theta)^2 + (v \sin \theta)^2}}$ is a number called the _magnitude_ of ${\vc{v}.}$ Given a vector ${\vc{v},}$ we may denote the ${\vc{v}}$ with the notation ${v_x \hat{i} + v_y \hat{j},}$ where ${\hat{i}}$ and ${\hat{j}}$ are placeholder variables called _unit vectors_, used purely to demarcate ${v_x}$ and ${v_y}$

> __~vector addition~.__ Let ${\vc{a}}$ and ${\vc{b}}$ be vectors. We define the operation of _vector addition_ as follows:
> 
> $$
> 	\vc{a} + \vc{b} = (\abs{a},a_x,a_y) + (\abs{b},b_x,b_y) = 
> 	\ar{\sqrt{(a_x + b_x)^2 + (a_y + b_y)^2},~a_x+b_x,~a_y + b_y}.
> $$

## Motion
> __~displacement~.__ Let ${\vc{r_f}}$ and ${\vc{r_i}}$ be physical vectors. We define _displacement_ as the vector ${\Delta \vc{r} = \vc{r_f} - \vc{r_i}.}$

> __~average velocity~.__ Given the closed interval ${\ix{a,b} \in \reals.}$ We define _average velocity_ as the vector
> 
> $$
> 	\vc{v}_{\avg} = \frac{\Delta \vc{r}}{\Delta t},
> $$
> 
> where ${\Delta\vc{r}}$ is displacement, and ${\Delta t = b - a,}$ called the _time interval_ of ${\vc{v}_{\avg}.}$

__~instantaneous velocity~.__ Where ${\Delta t}$ is a time interval, we define _instantaneous velocity_ as the vector

> $$
> 	\vc{v} = \ll{\Delta t}{0} \frac{\Delta \vc{r}}{\Delta t} = \di{\vc{r}}{t}.
> $$


## JavaScript Array Methods
> __~push~.__ Let ${A}$ be an array of ${n}$ elements, ${\ix{a_0, \ldots, a_{n-1}},}$ and let ${b}$ be a data object. Then the method ${A\mc{push}\px{b}}$ inserts ${b}$ at index ${n.}$ If ${A = \ix{~},}$ then ${b}$ is inserted at index ${0.}$

> __~pop~.__ Let ${A}$ be an array of ${n}$ elements, ${\ix{a_0, \ldots, a_{n-1}}.}$ Then the method ${A\mc{pop}\px{~}}$ removes the last element ${a_{n-1}}$ of ${A,}$ and returns ${a_{n-1}.}$


> __~shift~.__ Let ${A}$ be an array of ${n}$ elements, ${\ix{a_0, \ldots, a_{n-1}}.}$ Then the method ${A\mc{shift}\px{~}}$ removes the first element ${a_0}$ of ${A,}$ and returns ${a_0.}$


## Machine
## Chips
> __~definition~.__ A _chip_ is a function ${c: I \mapsto O,}$ where ${I}$ is a set of Boolean values, and ${O}$ is a set of Boolean values.

### Or-chip

<Grid cols={2}>

> __~or-chip~.__
> $$
> 	\df{or}(a,b) = \case{
> 		1 &\if a = 1 \\
> 		1 &\if b = 1 \\
> 		0 &\else
> 	}
> $$
> 
> Given a bitstring argument of ${n}$ bits ${a\ix{n}=\ix{a_1,\ldots,a_n}}$ and ${b\ix{n}=\ix{b_1,\ldots,b_n}}$
> 
> $$
> 	\small\df{or}(a\ix{n},b\ix{n}) = \case{
> 		\ix{\df{or}(a_1,b_1)} &\if n = 1 \\
> 		\df{or}(a\ix{n-1},b\ix{n-1})\uplus\df{or}(a_n,b_n) &\else
> 	}
> $$

<Fig
	link={"https://res.cloudinary.com/sublimis/image/upload/v1653001259/cs/or_gate.svg"}
	imwidth={"32"}
	imheight={"15"}
	caption={"or-gate"}
	width={"30"}
	layout={'responsive'}
	fit={""}
/>
</Grid>




~example~.
$$
	~\\
	\df{or}(\ix{1001},\ix{0101}) = \ax{
		&\yd{1}\sd{0}\sd{0}\yd{1} \\
		\df{or}~&\sd{0}\yd{1}\sd{0}\yd{1} \\ \hline
		&\yd{1}\yd{1}\sd{0}\yd{1} \\
	}
$$

### Not-chip
> __~not-chip~.__
> 
> $$
> 	\df{not}(x) = \case{
> 		0 &\if x = 1 \\
> 		1 &\if x = 0
> 	}
> $$
> Given a bitstring argument of ${n}$ bits ${x\ix{n}=\ix{x_1,\ldots,x_n},}$
> 
> $$
> 	\small\df{not}(x\ix{n}) = \case{
> 		\ix{\df{not}(x_1)} &\if n = 1 \\
> 		\df{not}(x\ix{n-1})\uplus\df{not}(x_n) &\else
> 	}
> $$

### And-chip
> __~and-chip~.__
> $$
> 	\df{and}(a,b) = \case{
> 		0 &\if a = 0 \\
> 		0 &\if b = 0 \\
> 		1 &\else
> 	}
> $$

### Nand-chip
> __~nand-chip~.__
> $$
> 	\df{nand}(a,b) = \case{
> 		0 &\if ~~\df{and}(a,b) = 1 \\
> 		1 &\else
> 	}
> $$

### Xor-chip
> __~xor-chip~.__
> $$
> 	\df{xor}(a,b) = \case{
> 		1 &\if ~~\df{and}(\df{not}(a), b) = 1 \\
> 		1 &\if ~~\df{and}(a, \df{not}(b)) = 1 \\
> 		0 &\else
> 	}
> $$

### Mux-chip
> __~mux-chip~.__
> $$
> 	\df{mux}(a,b,s) = \case{
> 		a &\if s=0 \\
> 		b &\else
> 	}
> $$

### Demux-chip
> __~demux-chip~.__
> $$
>  	\df{dmux}(x,s) = \case{
> 		(x,0) &\if s = 0 \\
> 		(0,x) &\if s = 1
>  	}
> $$


## Iteration and Orders of Growth
> __~notation~.__ Let ${f}$ be a function with ${x\in\tx{dom}\px{f} \subseteq \reals}$ and ${f(x)\in\tx{codom}\px{f} \subseteq \reals,}$ and let ${a}$ and ${b}$ be constants in ${\reals.}$ The notation ${\iter{x=a}{b}f(x),}$ or, equivalently,
> 
> $$
> 	\diter{x=a}{b}~f(x),
> $$
> 
> denotes ${b-a}$ evaluations of ${f.}$ We define ${\diter{x=a}{b}~f(x)=f(b).}$

<Grid cols={2}>

> ~example~. ${\diter{i=1}{5}i = 5.}$
> 
> | ~iteration~ | ${\diter{}{}}$ |
> | ----------- | -------------- |
> | 1           | 1              |
> | 2           | 2              |
> | 3           | 3              |
> | 4           | 4              |
> | 5           | 5              |


> ~example~. ${\diter{i=0}{5}(i+1) = 5.}$
> 
> | ${i}$ | ${\diter{}{}}$ |
> | ----- | -------------- |
> | 0     | 1              |
> | 1     | 2              |
> | 2     | 3              |
> | 3     | 4              |
> | 4     | 5              |


> ~example~. ${\diter{i=1}{5}\ar{ai} = a^5.}$
> 
> | ~iteration~ | ${\diter{}{}}$    |
> | ----------- | ----------------- |
> | 1           | ${a}$             |
> | 2           | ${a(a)}$          |
> | 3           | ${a(a(a))}$       |
> | 4           | ${a(a(a(a)))}$    |
> | 5           | ${a(a(a(a(a))))}$ |

> ~example~. ${\diter{i=1}{3}~i\ar{\diter{j=1}{3}~j}=9.}$
>
> | ${i}$ | ${j}$ | ${\diter{}{}}$ |
> | ----- | ----- | -------------- |
> | 1     | 1(1)  | 1              |
> |       | 1(2)  | 2              |
> |       | 1(3)  | 3              |
> | 2     | 2(1)  | 2              |
> |       | 2(2)  | 4              |
> |       | 2(3)  | 6              |
> | 3     | 3(1)  | 1              |
> |       | 3(2)  | 6              |
> |       | 3(3)  | 9              |

</Grid>

We will use the iteration notation to help us prove some theorems associated
with loops.





## Loop Execution Order
Before we turn to the theorems, let's be very clear about the order of execution for the various loop forms. The most common loop is the _for-loop_.

> __~for-loop~.__ Let ${m,n}$ be constants in ${\reals,}$ let ${C}$ be any constant function, and let ${\tx{dom}\px{f} \subseteq \reals}$ and ${\tx{codom}{\px{f}} \subseteq \reals.}$ Then the statement
> 
> $$
> 	{\bf for}\space(\let{i}{m}, \space i \lt n, \space f\px{i})\mapsto \set{C}
> $$
> 
> is called a _for-loop_. The statement ${\let{i}{a}}$ is executed first and exactly once. The test condition ${i \lt n}$ is performed first at each iteration, for a total of ${(n-m)+1}$ executions. The constant function ${C}$ is performed second at each iteration, for a total of ${n-m}$ executions. The function ${f}$ is performed last at each iteration, for a total of ${n-m}$ executions. 


> __~while-loop~.__ Let ${m,n}$ be constants in ${\reals,}$ let ${C}$ be any constant function, and let ${\tx{dom}\px{f} \subseteq \reals}$ and ${\tx{codom}{\px{f}} \subseteq \reals.}$ Then the statement
> 
> $$
> 	\let{i}{m},\space{\bf while}\space(i \lt n)\mapsto \set{C,\space f\px{i}}
> $$
> 
> is called a _while-loop_. The statement ${\let{i}{m}}$ is executed first and exactly once. The test condition ${i \lt n}$ is performed first at each iteration, for a total of ${(n-m)+1}$ executions. The constant function ${C}$ is performed second at each iteration, for a total of ${n-m}$ executions. The function ${f}$ is performed last at each iteration, for a total of ${n-m}$ executions.

In general, determining the time complexity of a for-loop execution amounts to a counting problem, or, more generally, a combinatorics problem.

~example~. How many times does the loop
${{\bf for}\space(\let{i}{0}, \space i \lt n, \space i\pl\pl)\mapsto \set{C}}$ 
execute? This nothing more than the sequence ${(0,1,2,\ldots,n-2,n-1).}$ We can count the number of terms of this sequence:

$$
	\ax{
		  &(0,1,2,\ldots,n-2,n-1) \\
		+ &(1,1,1,\ldots,\no{n-}1,\no{n-}1) \\ \hline
		  &(1,2,3,\ldots,n-1,\no{n-}n)
	}
$$

Hence, the loop executes ${n}$ times.

~example~. How many times does the loop
${{\bf for}\space(\let{i}{0}, \space i \lt n, \space i\pl\pl2)\mapsto \set{C}}$ 
execute? The sequence we'd normally expect is ${(0,1,2,\ldots,n-1).}$ We know that this executes ${n}$ times. But, because we're incrementing by 2, we're only interested in multiples of 2: ${(1,{\red2},3,{\red4},5,{\red 6},7,\ldots,n).}$ More explicitly: ${(2,4,6,8,\ldots,n).}$ What is the length of this sequence? We can divide by 2:

$$
	(2/2,4/4,6/2,8/2,\ldots,n/2) = (1,2,3,4,\ldots,n/2).
$$

Thus, the loop executes ${n/2}$ times.


### Cardinality
An alternative name for combinatorics is _finite set theory_ — it is the study of finite sets, with a particular emphasis on the cardinalities of finite sets.

> __~definition~.__ Given the sets ${A}$ and ${B,}$ we say that ${A}$ is _equipotent_ to ${B}$ if there exists a bijection from ${A}$ to ${B.}$ If ${A}$ and ${B}$ are equipotent, we write ${A \seteq B.}$

> __~definition~.__ Given the non-empty set ${A,}$ we say that ${A}$ is _countably infinite_ if, and only if, ${A \seteq B.}$

> __~definition~.__ Given the non-empty set ${A,}$ if ${A}$ is neither countably infinite nor countably finite, then we say that ${A}$ is _uncountably infinite_, and write ${\abs{A}=\infty.}$

> __~definition~.__ Given the set ${A,}$ we say that ${A}$ is _countably finite_ if, and only if, ${A \seteq \ix{n}}$ or ${A \seteq \nil.}$ If ${A \seteq \ix{n},}$ we write ${\abs{A} = n.}$ If ${A \seteq \nil,}$ we write ${\abs{A}=0.}$

> __~properties of cardinality~.__ Let ${A}$ and ${B}$ be countably finite sets. The following properties hold for ${A.}$
>
> (1) ${A \seteq A.}$
> 
> (2) If ${A \seteq B,}$ then ${B \seteq A.}$
> 
> (3) If ${A \seteq B}$ and ${B \seteq C,}$ then ${A \seteq C.}$

The definitions above evidence a preliminary task that's not often mentioned in combinatorics materials: Before we even deal with the cardinality of sets, we must first establish that the sets in question are actually finite.

### Disjoint Sum Rule
> __~theorem~.__ Let ${A}$ be a set of disjoint finite sets ${A_1, A_2, \ldots A_n.}$ It follows that
> 
> $$
> 	\abs{A} = \abs{A_1 \dup A_2 \dup \ldots \dup A_n} = \left\vert{\bigsqcup_{i=1}^{n}A_i}\right\vert = \dsum{i=1}{n}\abs{A_i}= \abs{A_1} + \abs{A_2} + \ldots + \abs{A_n}.
> $$

__~probability of disjoint events~.__ If an event can occur ${m}$ 

~example~. A box contains 18 ${\cash{1}}$ bills and 3 ${\cash{100}}$ bills. What is probability of selecting one ${\cash{100}}$ bill? There are ${18+3=21}$ bills: ${\pb{\tx{pick}~~\cash{1}} = 3/21 \approx 0.143.}$ 

### Complementary Rule
> __~theorem~.__ Let ${U}$ be a countably finite set, and let ${A \subseteq U.}$ It follows that:
> 
> $$
> 	\abs{\nix{A}} = \abs{U} - \abs{A}.
> $$

### Symmetry Rule 
> __~theorem~.__ Let ${U}$ be a countably finite set. If ${A \subseteq U,}$ ${B \subseteq U,}$ ${C \subseteq U,}$ and ${\abs{A} = \abs{B},}$ then
> 
> $$
> 	\abs{A} = \dfrac{\abs{U}-\abs{C}}{2}.
> $$

Visually, this rule results from the following. Suppose ${U}$ is a countably finite set partitioned into the sets ${A,B,C,}$ with ${\abs{A}=\abs{B}:}$

$$
	^{\normalsize U}~{\wd{A}\wd{~~C~~}\wd{B}}
$$

The cardinality of ${A}$ (or ${B}$) can be determined by constructing a set without ${C,}$ counting all its elements, and dividing by 2.

$$
	^{\normalsize U}~{\wd{A}{\sd{~~\no{C}~~}}\wd{B}}
$$

### Pigeonhole Principle
__~theorem~.__ Let ${P}$ and ${H}$ be countably finite sets, and let ${f: P \mapsto H.}$ If ${\abs{P} \gt \abs{H},}$ then at least one ${h \in H}$ is mapped to _twice_. That is, ${f}$ is _not injective_.

__~theorem~.__ Let ${P}$ and ${H}$ be countably finite sets, and let ${f: P \mapsto H.}$ If ${\abs{P} \lt \abs{H},}$ then at least one ${h \in H}$ is _not mapped to_. That is, ${f}$ is _not surjective_.

### Enumerating Pairs
> __~theorem~.__ Given ${n}$ elements ${\set{x_1,\ldots,x_n}}$ and ${m}$ elements ${\set{y_1,\ldots,y_n},}$ there are ${n \times m}$ _pairs_ of the form ${(x_j,y_k).}$ That is, tuples containing exactly one unique element from each type.

~example~. Suppose ${A = \set{1,2,3}}$ and ${B = \set{a,b,c}.}$ We have the pairs:

$$
	\lset{\ax{
		(1,a) & (2,a) & (3,a) \\
		(1,b) & (2,b) & (3,b) \\
		(1,c) & (2,c) & (3,c) \\
	}}
$$

### Enumerating Multiplets
> __~theorem~.__ Given ${n_1}$ elements ${\set{a_1, \ldots, a_{n_1}},}$ and ${n_2}$ elements ${\set{b_1, \ldots, b_{n_2}},}$ etc., up to ${n_r}$ elements ${\set{x_1, \ldots, x_{n_r}},}$ there are ${n_1 \times n_2 \times \ldots \times n_r}$ _multiplets_ of the form ${(a_{j_1}, b_{j_1}, \ldots, x_{j_r}).}$ That is, tuples containing ${r}$ unique elements from each type.

This is merely a generalization of the previous theorem. Pairs have an _arity_ of two: ${A \times B.}$ Multiplets have an arity of ${r:}$ ${S_1 \times S_2 \times \ldots \times S_r.}$

### Three Possible Cases of Orderings
> __~definition: generic ordering~.__ Let ${A}$ be the set ${\set{a_1,\ldots,a_n},}$ let ${B}$ be the set ${\set{b_1,\ldots,b_n},}$ and let ${f \subset A \times B.}$ If ${(a_i, b_k) \in f}$ and ${(a_j,b_k) \in f,}$ then we say that ${f}$ is a _generic ordering_, or _sampling with replacement_. That is, after an element ${b_k}$ has been mapped to by an element ${a_i,}$ another element ${a_j \neq a_i}$ _may_ map to ${b_k.}$ 

> __~definition: injective ordering~.__ Let ${A}$ be the set ${\set{a_1,\ldots,a_n},}$ let ${B}$ be the set ${\set{b_1,\ldots,b_n},}$ and let ${f \subset A \times B}$ be a mapping from ${A}$ to ${B.}$ If every element of ${B}$ is mapped to _at most once_, then ${f}$ is an _injective ordering_, or _sampling without replacement_.

> __~definition: surjective ordering~.__ Let ${A}$ be the set ${\set{a_1,\ldots,a_n},}$ let ${B}$ be the set ${\set{b_1,\ldots,b_n},}$ and let ${f \subset A \times B}$ be a mapping from ${A}$ to ${B.}$ If every element of ${B}$ is mapped to _at least once_, then ${f}$ is a _surjective ordering_, or _total sampling with replacement_.

> __~definition: bijective ordering~.__ Let ${A}$ be the set ${\set{a_1,\ldots,a_n},}$ let ${B}$ be the set ${\set{b_1,\ldots,b_n},}$ and let ${f \subset A \times B}$ be a mapping from ${A}$ to ${B.}$ If every element of ${B}$ is mapped to _exactly once_, then ${f}$ is a _bijective ordering_, or _total sampling without replacement_.

### Enumerating Bijective Orderings
> __~theorem~.__  Given ${n}$ elements, there are ${n! = n(n-1)(n-2)\ldots(2)(1)}$ possible bijective orderings.

~example~. How many ways are there to order the elements of ${\set{0,1}?}$ There are ${2! = 2}$ ways: ${(0,1)}$ and ${(1,0).}$

~example~. How many ways are there to order the outcomes ${\set{a,b,c}?}$ There are ${3! = 6}$ ways:

<div className={`TableMatrix`}>
| position | 0     | 1     | 2     |
| -------- | ----- | ----- | ----- |
|          | ${a}$ | ${b}$ | ${c}$ |
|          | ${a}$ | ${c}$ | ${b}$ |
|          | ${b}$ | ${a}$ | ${c}$ |
|          | ${b}$ | ${c}$ | ${a}$ |
|          | ${c}$ | ${a}$ | ${b}$ |
|          | ${c}$ | ${b}$ | ${a}$ |
</div>

Orders are merely _bijections_ from a set of natural numbers to a set of objects. Thus, each of the orders above are distinct sets of tuples.

$$
	P_1=\lset{(0,a)~~(1,b)~~(2,c)} \\
	P_2=\lset{(0,a)~~(1,c)~~(2,b)} \\
	P_3=\lset{(0,b)~~(1,a)~~(2,c)} \\
	P_4=\lset{(0,b)~~(1,c)~~(2,a)} \\
	P_5=\lset{(0,c)~~(1,a)~~(2,b)} \\
	P_6=\lset{(0,c)~~(1,b)~~(2,a)} \\
$$

Each set ${P_i}$ is a mathematical object called a _permutation_, and more generally, a type of _strict ordering_. The term strict comes from the fact that (1) once a given element of the base set ${\set{a,b,c}}$ has been mapped to, it cannot be mapped to again, and (2) every element of the set ${\set{a,b,c}}$ is mapped to at least once. For example, we will never have a set ${\set{(0,a), (1,a), (2,a)}.}$ Orderings free from these two conditions are called _weak orderings_. This brings us to the notion of sampling with and without replacement.

### Enumerating Injective Orderings
> __~theorem~.__ Let ${A = \set{a_1, a_2, \ldots, a_n},}$ let ${B = \set{b_1, b_2 \ldots, b_m},}$ and let ${\ix{A \inj B}}$ be the set of all possible injections from ${A}$ to ${B.}$ It follows that
> 
> $$
> 	\abs{\ix{A \inj B}} = \dfrac{m!}{(m-n)!} ~~~~~\tx{provided}~~ (n \le m).
> $$
> 
> If ${m \lt n,}$ then ${\ix{A \inj B} = \nil.}$

### Enumerating Surjective Orderings
> __~theorem~.__ Let ${A = \set{a_1, a_2, \ldots, a_n},}$ let ${B = \set{b_1, b_2 \ldots, b_m},}$ and let ${\ix{A \surj B}}$ be the set of all possible surjections from ${A}$ to ${B.}$ It follows that
> 
> $$
> 	\abs{\ix{A \surj B}} = \dsum{i=0}{n}(-1)^i \dbinom{n}{i}(n-i)^m ~~~~~\tx{provided}~~ (n \le m).
> $$
> 
> If ${m \lt n,}$ then ${\ix{A \surj B} = \nil.}$


## "Impossibilities"
> __~theorem~.__ ${\pb{\nil}=0.}$
> 
> ~proof~. Set theory tells us that ${\Omega \cup \nil = \Omega,}$ and that ${\Omega \cap \nil = \nil.}$ Hence, ${\pb{\nil} + \pb{\Omega}=\pb{\Omega}.}$ That is, ${\pb{\nil} + 1 = 1.}$

## Nonnegativity of Probability
> __~theorem~.__ ${\pb{A} \le 1}$ for all ${A \subseteq \Omega.}$
>
> ~proof~. Every event ${A}$ has a complete with ${\pb{\nix{A}} \ge 0.}$ It follows that ${\pb{A}=1-\pb{\nix{A}} \le 1.}$

## Probability of Not-Events
> __~theorem~.__ ${\pb{\nix{A}}=1-\pb{A}.}$
>
> ~proof~. ${A \cap \nix{A}=\nil}$ and ${A \cup \nix{A}=\Omega.}$ Therefore, ${\pb{\Omega}=\pb{A}+\pb{\nix{A}}=1.}$ Hence, ${\pb{\nix{A}}=1-\pb{A}.}$

## Probability of Or-Events
> __~theorem~.__ Let ${A}$ and ${B}$ be events of a sample ${\Omega.}$ The probability of _${A}$ or ${B}$ occurring_, denoted ${A \cup B,}$ is given by
> 
> $$
> 	\pb{A \cup B} = \pb{A} + \pb{B} - \pb{A \cap B}.
> $$


## Combinatorial Analysis
Combinatorics is a branch of mathematics distinct from probability, and contrary to popular belief, underlies a small subset of probability theory. Much of modern probability theory is instead built atop _measure theory_, the branch of mathematics from which integral calculus and geometry shoot. We cover only the basic combinatorial theorems necessary for the _combinatorial analyses_ of probability theory. We will also present the theorems without proof, as the proofs may be found in the [~note on combinatorics~](./../combinatorics/counting).

