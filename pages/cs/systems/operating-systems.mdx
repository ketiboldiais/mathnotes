# Operating Systems
_This note presents an overview of machines._

## Axioms
> __~axiom~.__ A computer system is a set
> 
> $$
> 	\Ss = \bigsqcup\limits_{i=1}^{n} C_i
> $$
> 
> where ${C}$ is a a _chip_.

~example~. A modern computer system may consist of several chips:

$$
\sd{\eqs{
	&\sd{\mo{CPU}}\dk\sd{\mo{GPU}}\dk\sd{\mo{RAM}}
	\\
	&\sd{\eqs{
		&\mo{DISK} \\
		&\sd{\mo{SSD}} \dk \sd{\mo{HDD}}
	}}
	\\
	&\sd{\eqs{
		&\mo{NETWORK} \\
		&\sd{\mo{WIFI}}\dk\sd{\mo{ETHERNET}}\\
		&\sd{\mo{BLUETOOTH}}
	}} \\
	&\sd{\mo{SOUND}}\dk\sd{\mo{KEYBOARD}} \\
	&\sd{\mo{PRINTER}}\dk\sd{\mo{USB}}
}}
$$

To operate the system, we can manipulate each chip manually. We cannot rely on the CPU for every operation, since each chip has a certain degree of independence. I.e., the network chip has its own set of operations, the sound chip has its own set of operations, the GPU has its own set of operations, etc.

> __~definition: application~.__ An _application_ is a structure ${(P,C)}$ where ${P}$ is a set of programs and ${C}$ is a set of computer system chips required to execute some program in ${P.}$

~example~. A rich text editor like Microsoft Word requires the keyboard chip, the GPU, RAM, the sound chip for alerts, the network chip for cloud storage, the CPU for computations, disk for storage, and many others.

> __~axiom~.__ Given a computer system ${\Ss,}$ there are more applications than there are chips.

> __~lemma~.__ Let ${H}$ be a set of chips in ${\Ss}$ and let ${A}$ be a set of applications reliant on ${\Ss.}$ At least one chip is used by two applications in ${A.}$

~proof~. Let ${f}$ be a map from ${A}$ to ${H,}$ where ${f}$ models the relationship _${A}$ uses ${H}$_. Given that ${\abs{A} \gt \abs{H},}$ by the Pigeonhole Principle, at least one chip in ${H}$ is mapped to twice. ${\dk \bs}$

> ~problem~. How do we ensure that an application's use of a chip ${C}$ does not conflict with another application's use of the chip ${C?}$

One solution is to use an __*operating system*__.

## Terminology
> __~definition: process~.__ A _process_ is an executing program.

~example~. Compiling a C program generates an executable. Running that executable creates a _process_.

> __~definition: thread~.__ A _thread_ is a state in the process.

~remark~. The first time we run an executable, a single thread is created. If our code creates another thread explicitly (or if we pass an argument during execution), that thread _spawns_ a new thread. That new thread is created with a __*system call*__.

~remark~. Whenever we execute a program, the OS creates a tiny, virtual computer with its own address space, its own files, its own sockets, etc. That tiny computer is called a _process_. Each instruction sequence in the program is called a _thread_. The program can have a single instruction sequence or many instruction sequences:

<Grid cols={2}>
$$
	\tx{---}
	\sd{\tx{1}}
	\sd{\tx{2}}
	\sd{\tx{3}}
	\tx{---}
	\\[2em]
	\tx{program with one thread}
$$

$$
	\tx{---}
	\sd{\tx{1a}}
	\sd{\tx{2a}}
	\sd{\tx{3a}}
	\tx{---}
	\\[1em]
	\tx{---}
	\sd{\tx{1b}}
	\sd{\tx{2b}}
	\sd{\tx{3b}}
	\tx{---}
	\\[2em]
	\tx{program with two threads}
$$
</Grid>

~detail~. All threads share the same resources allocated to the process. Thus, given processes ${P_1}$ and ${P_2,}$ threads of ${P_1}$ cannot use the resources of ${P_2.}$

~example~. Let ${T_1}$ and ${T_2}$ be threads in a process ${P.}$

| Shared by ${T_1}$ and ${T_2}$             | Private to ${T_1}$/${T_2}$ |
| ----------------------------------------- | -------------------------- |
| global variables                          | CPU registers              |
| heap                                      | program counter            |
| I/O state (file descriptors/sockets/etc.) | stack (temporary results)  |
| code (program instructions)               | thread metadata            |

Resources private to a thread are kept in a _TCP (Thread Control Block)_.

Threads are always in one of three states: (1) __*running*__ (currently executing), (2) __*ready*__ (can execute), or (3) __*blocked*__ (cannot execute). If a thread requires an I/O task to finish, the OS marks it as _blocked_. Once the I/O finishes, the OS marks it as ready.

> __~definition: scheduler~.__ Let ${T_1}$ and ${T_2}$ be tasks. A _scheduler_ is a program that determines which of ${T_1}$ or ${T_2}$ runs first, how much time to give ${T_1}$ for execution, and how much time to give ${T_2}$ for execution.

> __~definition: parallel programming~.__ The use of more than one processor to complete a task.

Parallel programming is also called _multiprocessing_.

> __~definition: multiprogramming~.__ The use of more than one process to complete a task.

> __~definition: singlethreading~.__ The use of one _thread_ to complete a task. 

> __~definition: multithreading~.__ The use of more than one _thread_ to complete a task.

To use multithreading, a program must explicitly create a thread (usually some method provided by a library).

~example~. The ${\mo{libc}}$ library provides an API for system calls. The OS library API for threads is ${\mo{pthreads}}$ (_POSIX threads_):


> __~definition: concurrent programming~.__ Two threads ${T_1}$ and ${T_2}$ are said to run _concurrently_ if the scheduler is free to determine whether ${T_1}$ runs before ${T_2}$ or whether ${T_1}$ and ${T_2}$ interleave.

<Grid cols={2}>
$$
\eqs{
	&\yd{a1}\yd{a2}\yd{a3} \\
	&\bd{b1}\bd{b2}\bd{b3}\bd{b3}\bd{b3}\\
	&\gd{c1}\gd{c2} \\
}\\[2em]
\tx{parallel programming}
$$
$$
\eqs{
	&\yd{a1}\bd{b1}\gd{c1}\yd{a2}\bd{b2}\gd{c2} \ldots \\
}\\[2em]
\tx{concurrent programming}
$$
</Grid>

Concurrent programming is _not_ parallel programming. A _concurrent program_ is a program that either interleaves tasks or schedules them arbitrarily. A _parallel program_ is a program that performs tasks _simultaneously_.


## Common Delays
| Task                                     | Delay                   |
| ---------------------------------------- | ----------------------- |
| L1 Cache read                            | 0.5 nanoseconds         |
| Branch misprediction                     | 5 nanoseconds           |
| L2 Cache read                            | 7 nanoseconds           |
| Mutex Lock/Unlock                        | 25 nanoseconds          |
| RAM read                                 | 100 nanoseconds         |
| Compress 1KB with Zippy                  | 3000 nanoseconds        |
| Send 2KB over 1 Gbps network             | 20,000 nanoseconds      |
| Read 1MB sequentially from memory        | 250,000 nanoseconds     |
| Round trip within a datacenter           | 500,000 nanoseconds     |
| Disk seek                                | 10,000,000 nanoseconds  |
| Read 1MB seqentially from disk           | 20,000,000 nanseconds   |
| Send a packet from California to Holland | 150,000,000 nanoseconds |


## Challenges in Multithreading
__~nondeterministic threads~.__ The program's threads share state (the threads depend on common conditions), so the scheduling is left to the OS's scheduler — the scheduler is free to run in any order and switch at any time. Such programs are difficult to benchmark/test.

__~deterministic threads~.__ The threads share no state whatsoever. Easier benchmark and test, but not always achievable because of the problem's parameters or other factors (e.g., its much easier and less expensive for the threads to share state).

~example~. Let ${x = 0}$ and let ${y = 0.}$ If ${\tx{thread A} \mapsto f(x) = 1 + x}$ and ${\tx{thread b} \mapsto f(y) = y + 2,}$ then threads ${A}$ and ${B}$ are deterministic. The outcome of one doesn't depend on the other.

~example~. Let ${x = 0}$ and let ${y = 0.}$ If ${\tx{thread A} \mapsto f(x) = y + 1}$ and ${\tx{thread b} \mapsto f(y) = y:=2, 2y.}$ then threads ${A}$ and ${B}$ are nondeterministic: ${x}$ could be 1, 3, or 5. There's a _race condition_ — ${A}$ races against ${B.}$

__~synchronous threads~.__ Threads ${T_1}$ and ${T_2}$ are _synchronous threads_ if (1) both threads have a shared state, and (2) both threads are coordinated to prevent race conditions.

__~lock~.__ A data object that only one thread can hold at a time.

__~mutual exclusion~.__ A restriction that only one thread performs one task at a time.

We can achieve mutual exclusion using locks. If a thread _acquires_ a lock, then no other thread has that lock. If we have a rule that only the lock holder performs task ${x,}$ then we ensure that only one thread performs the task ${x.}$ Once that thread _releases_ the lock, then the other threads wake up and acquire the lock.

__~critical section~.__ One or more instructions that exactly one thread is permitted to execute. 

## Forking
> __~fork~.__ Let ${A}$ and ${B}$ be address spaces, and let ${P}$ be a process running in ${A.}$ A _fork_ of ${P}$ is a copy of ${P}$ (a new process with a different PID and a single thread initially) into ${B.}$

~remark~. Fork returns a ${\mo{pid}}$ (essentially an integer). If ${\mo{pid} \gt 0,}$ then we're running in the original parent process. If ${\mo{pid} = 0,}$ then we're running in a new child process. If ${\mo{pid} \lt 0,}$ then we're running in the original process.

~example~. 
<Algo>

1. ${\tx{int}}$ ${i;}$
2. ${\tx{pid\_t}}$ ${cpid \eq \tx{fork}\px{~};}$
3. __if__ ${\px{cpid \gt 0}}$ `{` _parent process_
	1. __for__ ${\px{i \eq 0; \dk i \lt 10; \dk i\inc1}}$ `{`
		1. ${\tx{printf}\px{\string{Parent: \%d \texttt{\textbackslash}n}, i};}$
	2. `}`
4. `}`
5. __else if__ ${\px{cpid ~\mo{==}~ 0}}$ `{` _child process_
	1. __for__ ${\px{i \eq 0; \dk i \gt \mi 10; \dk i\dec1}}$ `{`
		1. ${\tx{printf}\px{\string{Parent: \%d \texttt{\textbackslash}n}, i};}$
	2. `}`
6. `}`
7. __else__ `{` ${\textit{handle error}}$ `}`

</Algo>

Forking is a common pattern for shells. Let ${C}$ be a shell command executing a program ${P.}$ Within ${P,}$ the code appears as follows:

<Algo>

1. ${pid = \tx{fork}\px{~}}$
2. __if__ ${\px{pid \mo{==} 0}}$
	1. ${\tx{exec}\px{\ldots};}$
3. __else__
	1. ${\tx{wait}(\band stat)}$

</Algo>

Line 0 indicates that ${pid}$ is a fork. Lines 1 through 2 create a child process, and lines 3 to 4 create a parent process. The parent process does not execute (it _waits_) until the child process has finished. That child process may go off and execute some other program. Once the child process has finished, the parent process begins executing.

### Process Management API
| Syntax             | Semantics                                                                     |
| ------------------ | ----------------------------------------------------------------------------- |
| ${\tx{exit}}$      | Terminate the process                                                         |
| ${\tx{fork}}$      | Copy the process                                                              |
| ${\tx{exec}}$      | Change the program run by the current process                                 |
| ${\tx{wait}}$      | Wait for a proces to finish                                                   |
| ${\tx{kill}}$      | Send a signal (akin to an interrupt) to another process                       |
| ${\tx{sigaction}}$ | Set handlers for signals (what the process should do if it receives a signal) |


~example~. Below, the parent process does not execute code block ${A}$ until it receives the status code from the child process. That status code isn't sent to the parent process until the child process finishes executing code block ${B.}$ There are many ways to use the _wait_ instruction. Below, the _wait_ instruction only applies to the next child.

<Algo>

1. ${\tx{int} \dk status;}$
2. ${\tx{pid\_t}\dk tcpid;}$
3. ${cpid = \tx{fork}\px{~}}$
4. __if__ ${cpid \gt 0}$ `{` _parent process_
	1. ${\sd{code \dk block \dk A}}$
	2. ${tcpid = \tx{wait}\px{\tx{\band{status}}}}$
5. `}`
6. __else if__ ${\px{cpid \mo{==} 0}}$ `{` _child process_
	1. ${\sd{code\dk block \dk B}}$
	2. ${\tx{exit}\px{42};}$
7. `}`
8. __else__ `{`
	1. ${\sd{handle \dk error}}$
9. `}`

</Algo>

### Signaling
Two processes ${P_1}$ and ${P_2}$ can communicate with one another with _signals_. This is provided by _sigaction_.

~example~. 

<Algo>

1. ${\mo{\#include } \lt\mo{stdlib.h}\gt}$
2. ${\mo{\#include } \lt\mo{stdio.h}\gt}$
3. ${\mo{\#include } \lt\mo{sys/types.h}\gt}$
4. ${\mo{\#include } \lt\mo{unistd.h}\gt}$
5. ${\mo{\#include } \lt\mo{signal.h}\gt}$
6. ${~}$
7. __void__ ${\tx{signalCallbackHandler}\px{\tx{int}\dk signum}}$ `{`
	1. ${\sd{code \dk block A}}$
	2. ${\tx{exit}\px{1};}$
8. `}`
9. __int__ ${\tx{main}\px{~}}$ `{`
	1. ${\tx{struct} \dk sigaction \dk sa;}$
	2. ${sa \mc \tx{sa\_flags} = 0;}$
	3. ${\tx{sigemptyset}\px{\band{sa}.\tx{sa\_mask}};}$
	4. ${sa\mc\tx{sa\_handler} = \tx{signalCallbackHandler};}$
	5. ${\tx{sigaction}\px{\df{sigint}, \band{sa}, \df{null}};}$
10. `}`

</Algo>

## POSIX
__*POSIX (Portable Operating System Interface _for UNIX_)*__ is an interface specification for Unix systems. The interface's axiom: Everything is a _file_. And because everything is a file, every communication to the kernel is done with the following interface:

| Method               | Semantics    |
| -------------------- | ------------ |
| ${\tx{open}\px{~}}$  | Open a file  |
| ${\tx{read}\px{~}}$  | Read a file  |
| ${\tx{write}\px{~}}$ | Write a file |
| ${\tx{close}\px{~}}$ | Close a file |

> __~definition: file~.__ A _file_ is a structure ${(D,B,M),}$ where ${D}$ is a unique name called a _directory_, ${B}$ is a sequence of bytes, and ${M}$ is a set of metadata.

~detail~. Keyboard strokes are sequences of bytes, sound emissions are sequences of bytes. Files differ from these sequences because they have unique identifiers and metada.



### Directories
~file metadata~. The file's metadata is a set of information, including but not limited to: file size, modification time, owner, security information (e.g., encryption protocol), and access control (e.g., read/write privileges).

~hierarchy~. On modern operating systems, file names are modelled as _graphs_, hence the term _file path_. The original UNIX systems modelled names with _trees_, hence the remnants of file-related terms like _siblings_, _children_, and _parents_ (e.g., _child process_ and _parent process_).

~current working directory~. Every process has a __*current working directory (CWD)*__. The process's CWD can be set with the system call

$$
	\sd{\tx{int} \dk \tx{chdir}\px{\tx{const} \dk \tx{char*} \dk path};}
$$

> __~absolute path~__. Let ${\df{path}_p}$ be the path to a file ${p.}$ We say that ${\df{path}_p}$ is _absolute_ if it specifies ${p}$ without the current working directory.

~example~. `/src/main.c`

~example~. `~/config.py` specifies the file `config.py` in the home directory. The symbol `~` means _home directory_, and it can be made relative (this is a function of the shell, not the operating system).


> __~relative path~__. Let ${\df{path}_p}$ be a filepath to a process ${p.}$ We say that ${\df{path}_p}$ is _relative_ if it specifies ${p}$ with the current working directory.

~example~. `./index.html` specifies the file `index.html` in the current working directory.

~example~. `../dropdown.js` specifies the file `dropdown.js` in the parent directory.

~example~. `~guest/note.txt` specifies the file `note.txt` in the home directory of the `guest` account.

## I/O and Storage
### Bytestreams
> __~definition: bytestream~.__ A _bytestream_ is an unformatted sequence of bytes with an memory address (pointer).

~example~. 

<Algo>

1. ${\mo{\#include <{stdio.h}>}}$
1. ${\df{file}\mo{*} \dk\tx{fopen}\px{\tx{const} \dk \tx{char} \dk \mo{*}{filename}, \dk \tx{const}\dk \tx{char}\mo{*} \dk mode};}$
2. ${\tx{int}}$ ${\tx{fclose}\px{\df{file}\mo{*} \dk fp};}$

</Algo>

The method ${\tx{fopen}}$ returns a pointer to the sequence of bytes with the name ${filename.}$ The ${mode}$ is a string value that specifies what will be done to the file.

| ${mode}$ Value       | Semantic                                                                                                  |
| -------------------- | --------------------------------------------------------------------------------------------------------- |
| ${\string{r}}$       | ${filename}$ exists, open it for reading                                                                  |
| ${\string{w}}$       | Open ${filename}$ and write to it (overwrites existing bytes). If ${filename}$ does not exist, create it. |
| ${\string{a}}$       | Open ${filename}$ for appending (write without overwriting). If ${filename}$ does not exist, create it.   |
| ${\string{r\mo{+}}}$ | ${filename}$ exists, open it for reading and writing                                                      |
| ${\string{w\mo{+}}}$ | Create a new empty file named ${filename,}$ read and write to it.                                         |
| ${\string{a\mo{+}}}$ | Open ${filename}$ for reading and appending.                                                              |

~detail~.  C's file API returns the null pointer for errors. When opening a file, we must always check whether the pointer returned is null.

### Predefined Streams 
| Stream                              | Semantic                              |
| ----------------------------------- | ------------------------------------- |
| ${\df{file}\mo{*} \dk \tx{stdin}}$  | Program input bytestream              |
| ${\df{file}\mo{*} \dk \tx{stdout}}$ | Program output bytestream             |
| ${\df{file}\mo{*} \dk \tx{stderr}}$ | Program diagnostics/errors bytestream |

### Interprocess Communication
~point~. On a UNIX system, processes may communicate with one another through C's file API.

| Character-oriented Methods                                                                           | Semantic                                                                   |
| ---------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| ${\tx{int}\dk\tx{fputc}\px{\tx{int}\dk c, \dk \df{file}\mo{*}\dk fp};}$                              | Writes the character ${c}$ to the bytestream ${fp}$                        |
| ${\tx{int}\dk\tx{fputs}\px{\tx{const char}\mo{*}\dk s, \dk \df{file}\mo{*} \dk fp};}$                | Writes the string ${c}$ to the bytestream ${fp}$                           |
| ${\tx{int}\dk\tx{fgetc}\px{\df{file}\mo{*} \dk fp};}$                                                | Returns the next byte in ${fp,}$ and advances the position pointer         |
| ${\tx{char}\mo{*} \dk \tx{fgets}\px{\tx{char}\mo{*} \dk s, \tx{int} \dk n, \df{file}\mo{*} \dk fp}}$ | Reads the line ${n}$ in ${fp}$ and stores it in the string (buffer) ${s.}$ |


| Block-oriented Methods                                                                                                 | Semantic                                                                                |
| ---------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| ${\tx{size\_t}}$ fwrite(const void* ${A,}$ ${\tx{size\_t}}$ ${S,}$ ${\tx{size\_t}}$ ${n,}$ ${\df{file}}$ ${filename}$) | Writes ${n}$ bytes from ${filename}$ to the array of size ${S}$ with the pointer ${P.}$ |
| ${\tx{size\_t}}$ fread(void* ${A,}$ ${\tx{size\_t}}$ ${S,}$ ${\tx{size\_t}}$ ${n,}$ ${\df{file}}$ ${filename}$)        | Reads ${n}$ bytes from ${filename}$ to the array of size ${S}$ with the pointer ${P.}$  |

| Pointer-oriented Methods | Semantic                                                           |
| ------------------------ | ------------------------------------------------------------------ |
| fseek                    | Offsets the bytestream pointer (where to read next)                |
| ftell                    | Returns the bytestream pointer (location of current read)          |
| rewind                   | Returns the bytestream pointer to the first byte of the bytestream |

### File Descriptors 
C's file APIs are wrappers for POSIX system calls. In particular, `fopen` maps to the system call `open`, `fclose` maps to the system call `close`. When we call `fopen`, we implicitly call `open`. The kernel takes this call and returns an integer ${n}$ called a __*file descriptor*__. If ${n \lt 0,}$ then an error occurred and a global error variable is set for all subsequent threads. Otherwise, the `fopen` call's corresponding process takes the integer and places it in a table called the _file descriptor table_.

The integer returned from the kernel is an index of a system-wide table (maintained by the kernel) for all open files. This table is called the _open file table_. If there are no issues, the kernel inserts an entry into the table corresponding to the file. The kernel then returns that entry's index (the integer) to `fopen`. Notice: `fopen` does not return a pointer to a raw address in memory. Instead, it returns a pointer to an integer in the process's allocated, protected memory, which maps to an index in the kernel's open file table. This level of indirection ensures that one process does not access the bytestreams for another process.

~detail~.  The ${\df{file}\mo{*}}$ is a pointer to a `struct` containing (1) the _file descriptor_, (2) a _buffer_ (array) for incoming bytes, and (3) a _lock_ (in case multiple threads use the file concurrently).


### Kernel Buffers
When we read or write data, the kernel does not operate on individual bytes. Instead, it operates on _blocks_ of bytes. This is done not just because of performance and scheduling, but also because _disks_ operate in terms of blocks, not bytes. Because the kernel operates in terms of blocks, it must _buffer_ reads and writes.

Notice: This regulates how processes can speak to one another. If a process ${P_1}$ wants to speak to another process ${P_2,}$ ${P_1}$ must write to a file and ${P_2}$ must read from the file. That communication has a speed limit: The size of the kernel's buffer for the read and write calls.

To speed up interprocess communication, we can use a _memory buffer_: (1) Create a queue ${Q.}$ (2) ${P_1}$ writes bytes to ${Q.}$ (3) When ${Q}$ contains bytes and ${P_2}$ is ready, ${P_2}$ reads the data from ${Q.}$ The C API provides a method implementing this pattern:

$$
	\tx{int} \dk \tx{pipe}\px{\tx{int} \dk F\ix{2}};
$$

where ${F}$ is an integer array of length two. Suppose ${P_1}$ makes this call. Two file descriptors, integers ${a}$ and ${b,}$ are allocated for ${P_1.}$ The integer ${a}$ is stored in ${F\ix{0}}$ and the integer ${b}$ is stored in ${F\ix{1}.}$ If ${P_2}$ is a child process of ${P_1,}$ then ${P_1}$ and ${P_2}$ both hold the file descriptors ${a}$ and ${b}$ (but are still different processes with their own threads and their own protected memory). For ${P_1}$ to communicate to ${P_2,}$ ${P_1}$ writes to ${F\ix{1},}$ and ${P_2}$ reads from ${F\ix{0}.}$ If ${P_2}$ wants to communicate with ${P_1,}$ then ${P_2}$ writes to ${F\ix{1}}$ and ${P_1}$ reads from ${F\ix{0}.}$ We can ensure that ${P_1}$ and ${P_2}$ take turns reading and writing in coordination by imposing the following restriction: If ${P_1}$ is executing, ${P_2}$ must wait for ${P_1}$ to exit. If ${P_2}$ is executing, ${P_1}$ must wait for ${P_2}$ to exit.

## Protocols
The bytes of a memory buffer are just ones and zeroes. Nothing more. Thus, for processes ${P_1}$ and ${P_2}$ to communicate, there must be a predetermined set of rules for interpreting those ones and zeroes. In particular, rules for __*syntax*__ (how the communication is structured) and __*semantics*__ (how a particular structure should be interpreted). We implement syntax by specifying the set of possible bytestream reads (e.g., `100101`), and semantics by specifying what bytestream should be written when a particular bytestream is read.

### Networks
> __~definition: network connection~.__ Let ${X}$ and ${Y}$ be queues, such that: (1) ${A}$ enqueues bytes to ${X}$ and ${B}$ dequeues bytes from ${X,}$ and (2) ${B}$ enqueues bytes to ${Y}$ and ${A}$ dequeues bytes from ${Y.}$ The bytestream transferred between ${A}$ and ${B}$ through ${X}$ and ${Y}$ is called a _network connection_. We call ${X}$ the output queue of ${A}$ and ${Y}$ the input queue of ${A,}$ and ${Y}$ the output queue of ${B}$ and ${X}$ the input queue of ${B.}$ If a network connection exists between ${A}$ and ${B,}$ we say that ${A}$ and ${B.}$


> __~definition: web server~.__ A _web server_ is an application that serves files through a network connection.

~remark~. The processes ${A}$ and ${B}$ may be on the same machine, or they may be on entirely separate machines, possibly thousands of miles apart. In the latter case, suppose ${A}$ lives on ${M_1}$ and ${B}$ lives on ${M_4.}$ ${A}$ and ${B}$ are connected via through the processes on intermediary machines: ${A}$ of ${M_1}$ connects to ${B}$ of ${M_2,}$ ${B}$ of ${M_2}$ connects to ${A}$ of ${M_3,}$ ${A}$ of ${M_3}$ connects to ${B}$ of ${M_4.}$

#### TCP Sockets
Think of a simple console program that takes user input and returns screen outputs through `stdin` and `stdout`. For such a program, the input is the keyboard and the ouput is the screen. Webservers operate on the same principles, but with different endpoints: The input and output is a system _port_ (an entry point for network connections). A __*socket*__ is an interface that makes communications on different machines look similar to basic file I/O.

Sockets have two core methods: (1) `write` for enqueuing data to the output queue, and (2) `read` for dequeuing data from the input queue.

~example~. Let ${A}$ be a process on machine in California, and let ${B}$ be a process on a machine in Virginia. We call ${A}$ the __*client*__ and ${B}$ the __*server*__. When the server runs, it immediately calls the socket method `read`. If there is no data in the queue, ${B}$ waits. Suppose the user on the client side asks for a file ${f}$ on some network application. That application opens a socket, and calls the socket method `write`. Specifically, it writes to the socket's output queue the bytes comprising the request. The socket sends these bytes across the network to ${B}$'s input queue. ${B}$ then reads the bytes, and responds similarly: Write the response to the output queue, which the socket sends across the network back to ${A.}$

~problem i~. If ${A}$ and ${B}$ are on different machines, then they exist in different kernels. How can ${A}$ identify ${B}$ among potentially billions of machines?

~solution~. Ensure that ${A}$ and ${B}$ comply with a common protocol. Two common protocols are _Internet Protocol version 4 (IPv4)_ and _Internet Protocol version 6 (IPv6)_. Under IPv4, machines are identified by 32-bit integer, and under IPv6, a 128-bit integer. These integers are assigned by the network service provider.

~problem ii~. ${A}$'s machine can find ${B}$'s machine, but ${B}$ may be just one process among hundreds. Likewise, ${B}$'s machine can find ${A}$'s machine, but ${A}$ may be just one process among hundreds. How do they identify one another?

~remark~. Every tab on a web browser is a separate process.

~solution~. ${A}$'s operating system assigns ${A}$ a _port number_ and ${B}$'s operating system assigns ${B}$ a port number. On modern UNIX systems, port numbers 0 to 1023 are _system ports_ and can only be used with superuser (`sudo`) privileges. Ports 1024 to 49151 are registered ports and available for users. Ports 49152 to 65535 are _private ports_.

When the web server application first runs, it requests access to a special type of socket called a _server socket_. Each of the common port numbers (e.g., 20, 21, 22, 25, 53, 80, 123, 179, etc.) have an associated server socket. The client sends a _request to connect_, specifying the IP-address of ${B}$'s machine and one of the common ports, alongside a random port number identifying ${A,}$ generated by its operating system. The port specified depends on what the server process's protocol.

| Port   | Protocol/Service                                                                                                |
| ------ | --------------------------------------------------------------------------------------------------------------- |
| 20, 21 | FTP (File Transfer Protocol) for creating/deleting/reading/writing files on the server                          |
| 22     | SSH (Secure Shell) for secure network connections (e.g., remote logins)                                         |
| 25     | SMTP (Simple Mail Transfer Protocol) for email                                                                  |
| 53     | DNS (Doman Name System) for matching human-readable domain names (`cnn.com`) to IP addresses (`157.166.226.25`) |
| 80     | HTTP (Hypertext Transfer Protocol) for transferring hypertext (bytes understood by a browser)                   |
| 443    | HTTPS (Hypertext Transfer Protocol Secure) for transferring hypertext securely                                  |
| 123    | NTP (Network Time Protocol) for syncing computer clocks (critical for encryption)                               |
| 179    | BGP (Border Gateway Protocol) for establishing network routes                                                   |

The initial request sent is called an _acknowledgment request_. This is not a request for data. It's just a request to connect. That request travels to the server socket, and the web server application determines whether to accept the connection or not. If the web server accepts, it indicates so with the server socket, and the kernel creates a _connection socket_. ${A}$ reads and writes bytes through its socket, and ${B}$ reads and writes bytes through the connection socket.

<Algo>

1. ${\tx{char}\mo{*} \dk portname}$ _create socket, listen for connections_
2. ${\tx{struct} \dk \tx{addressInfo}\mo{*} \dk server = \tx{setAddress}\px{portname}}$
3. ${\tx{int} \dk {serverSocket} = \tx{socket}\px{{server}\rarr \tx{ai\_family}, \dk {server}\rarr \tx{ai\_socketype}, \dk \tx{server}\rarr \tx{ai\_protocol}};}$
4. ${\tx{bind}\px{{serverSocket}, \dk {server}\rarr \tx{ai\_addr}, \dk{server} \rarr \tx{ai\_addrlen}};}$
5. ${\tx{listen}\px{{serverSocket}, \dk \tx{MAX\_QUEUE}};}$
6. __while__ ${\px{1}}$ `{`
	1. ${\tx{int} \dk {connectedSocket} = \tx{accept}\px{{serverSocket}, \df{null}, \df{null}  };}$
	2. ${\tx{serve\_client}\px{{connectedSocket}};}$
	3. ${\tx{close}\px{{connectedSocket}};}$
7. `}`

</Algo>

~problem~. The server above handles each connection one at a time. This is extremely inefficient, especially if we consider a server that receives millions of requests. We can speed this up by forking.

### Child and Parent Processes in TCP Networks
It's critical to keep the abstractions of communications separate. We can think of a network communication as akin to an apartment ${A}$ with 12 kids sending letters to their 12 cousins in house ${B.}$. The kids are the _processes_. The letters are the _bytestreams_. The houses are the _machines_. The _network protocol_ (HTTPS/FTP/SMTP/etc.) is the postal service. The _transport protocol_ is the parent at house ${A}$ collecting all the letters from the kids and addressing them, and the parent at house ${B}$ receiving all the letters and distributing them.

Following this analogy, we can rewrite the server protocol's loop as follows:

<Algo>

1. __while__ ${\px{1}}$ `{`
	1. int ${connectedSocket}$ `=` accept(${serverSocket}$, ~null~, ~null);
	2. pid_t pid `=` fork();
	3. __if__ (pid `==` 0) `{`
		1. close(${serverSocket}$);
		2. serveClient(${connectedSocket}$);
		3. close(${connectedSocket}$);
	4. `}`
	5. __else__ `{`
		1. close(${conectedSocket}$)
		2. wait(~null~);
	6. `}`
2. `}`

</Algo>

The problem with this approach, however, is that the parent server process must wait for each child process to terminate before servicing the next. _Concurrency_ allows the server to handle new connections before the child process terminates. If the server can do so, then the server can now have multiple child processes serving requests concurrently:

<Algo>

1. __while__ ${\px{1}}$ `{`
	1. int ${connectedSocket}$ `=` accept(${serverSocket}$, ~null~, ~null);
	2. pid_t pid `=` fork();
	3. __if__ (pid `==` 0) `{`
		1. close(${serverSocket}$);
		2. serveClient(${connectedSocket}$);
		3. close(${connectedSocket}$);
		4. exit(0);
	4. `}`
	5. __else__ `{`
		1. close(${conectedSocket}$)
	6. `}`
2. `}`
3. close(${serveSocket}$);

</Algo>

Doing so, each child process gets a socket. Remember that the sockets created for the child processes are not server sockets. The _server socket_ is used only for listening, and only the parent process that has access to that socket. The parent process uses the server socket to accept or deny connection requests. If the parent process accepts, it _forks_ a new child process. That child process has a socket of its own — called the _connection socket_ — which maps to a particular port other than the server socket's port (e.g., the socket _will not_ be mapped to port 80). The parent process then returns an acknowledgment to the client process, instructing the client process to use the child process's port number.

### TCP Connection Sockets
The connection sockets are used for serving requests. The child processes write to their sockets the appropriate bytestreams in response to client requests.  Once they've responded, that child process finishes, the socket is closed, and a new request is handled. The bytestream written by the child process is the data requested by the client.

Every connection socket is identified by quadruple ${\px{\mo{IP}_s,\mo{IP}_d,\mo{P}_d,\mo{P}_s }}$ where ${\mo{IP}_s}$ is the source IP address, ${\mo{IP}_d}$ is the destination IP address, ${\mo{P}_d}$ is the destination port number, and ${\mo{P}_s}$ is the source port number. The bytes comprising these components, alongside the message (substantive data), are added to the bytestream as it moves from the socket down to the network interface. The child process writes to the socket the bytestream comprising the message. The socket takes the bytestream and includes the destination and source port numbers. The kernel takes this bytestream and sends it to the network driver. The network driver attaches the IP addresses, sends it to the link interface, which places it on a physical medium. We call this __*multiplexing*__.

When the recipient host receives the bytestream, it uses the 4-tuple mentioned earlier to identify which process the bytestream must go to. We call this __*demultiplexing*__.

### TCP and UDP 
Socket APIs are designed according to a transport protocol (a specification of syntax and semantics). All of our previous discusson assumes a __*TCP (Transmission Control Protocol)*__ socket — data is transferred between processes via bytestreams. There are, however, __*UDP (Unreliable Datagram Protocol)*__ sockets.

~key difference~. First, TCP sockets require a __*handshake*__ between the sender and the receiver before any bytestreams are sent between them (the initial connection request from the sender and denial/acceptance by the receiver). UDP sockets _do not_ require this handshake. Second, TCP sockets will purposely slow down request/response trasmissions. This is discussed in a later section, but it serves as a means of proactively reducing network congestion (i.e., if everyone drived a little less, we'd get to where we want faster and reduce the number of accidents). UDP sockts _do not_ impose any speed limit. They will send bytestreams as fast as possible (even if it means overwhelming an intermediary machine or the recipient).

| UDP                                                                                                                                          | TCP                                                                                                                                              |
| -------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| Bare bones                                                                                                                                   | Elaborate                                                                                                                                        |
| No guarantee that segments are delivered, let alone in order                                                                                 | Guaranteed that the segments are delivered in order                                                                                              |
| No handshake required                                                                                                                        | Handshake required                                                                                                                               |
| Simple to implement                                                                                                                          | For large servers, complex to implement                                                                                                          |
| Small headers                                                                                                                                | Large headers                                                                                                                                    |
| Less processing overhead                                                                                                                     | More processing overhead                                                                                                                         |
| No congestion controls                                                                                                                       | Congestion controls enforced                                                                                                                     |
| Uses: Tunneling/VPN, Skype, Zoom, VoIP, DNS. Generally, applications that do not require the client to receive all transmitted data at once. | Web, SSH, FTP, Telnet, SMTP (sending email), IMAP/POP (receiving email). Generally, applications where the client must receive all data at once. |
 
A third different: UDP sockets are identified by a quadruple
${(\mo{P}_s, \mo{P}_d, \mo{L}, \mo{C}),}$ where ${\mo{P}_s}$ and ${\mo{P}_d}$
are source and destination ports, ${\mo{L}}$ is the length of the UDP segment including the header, and ${C}$ is a bitstring called the _checksum_, used for detecting errors. If any bit in the checksum is flipped, then the receiving socket concludes that an error occurred. These bits can flip as they travel through the wire because of noise or attenuation.

~remark~. In networking, the term "guaranteed" is synonymous with correctness. It does not mean that bytes _will always arrive and arrive in order_. Guaranteed means, _The bytes should arrive and arrive in order, but if they do not, you will be notified_. 

#### UDP Checksums
The checksum works as follows: The sender's UDP socket treats a segment as  16-bit integer. The socket executes a function ${f}$ with the segment as an argument, and ${f}$ returns some ${n}$ bits (e.g., ten bits). Those ten bits are attached as the last bits of the segment. When the receiver's socket gets the segment, it uses the same function ${f}$ on the segment. If the returned bits do not match the last ${n}$ bits, then the receiver concludes that an error occurred.

### Packets


### Reliable Data Transfers
Data transfers are measured in terms __*reliability*__. This is a binary metric:

1. Did the data transfer deliver all the bytes?
	1. Yes ${\nc}$ Were all the bytes in order?
		1. Yes ${\nc}$ The data transfer is reliable.
		2. No ${\nc}$ Was the receiver notified that the bytes aren't in order?
			1. Yes ${\nc}$ Reliable data transfer
			2. No ${\nc}$ Unreliable data transfer
	2. No ${\nc}$ Was the receiver notified that not all bytes were delivered?
		1. Yes ${\nc}$ Reliable data transfer
		2. No ${\nc}$ Unreliable data transfer

If a receiver concludes that an error occurred, it returns to the sender a bitstring called a _NACK (No Acknowledgment)_. This tells the sender that the the last segment did not deliver correctly, so the sender retransmits. If no error occurred, then the receiver sends an _ACK (Acknowledgment)_. This tells the sender that the last segment deliver successfully, so the sender transmits the next segments.

## Synchronization 
The kernel represents a process as a _process control block (PCB)_. This is a set of information, including but not limited to: the process's status (running, ready, or blocked), the process's register state (when the process isn't ready), a PID (process ID identifying the process), execution time, memory space, and translation map.

All PCB's are kept in a data structure maintained by the kernel's __*scheduler*__. The scheduler decides which process gets CPU access next, as well as which process gets RAM access next. We call the event of switching from a process ${P_1}$ to a process ${P_2}$ a __*context switch*__. Consider a user typing code on Emacs while watching a tutorial on YouTube. Those are two different processes. Setting aside multicore processors, the Emacs process and the web browser tab process are two independent processes. They _do not_ execute simultaneously. Instead, the kernel switches CPU and RAM access quickly enough to create an _illusion_ of simultaneous execution. Timing these switches is a major challenge for OS designers. If the kernel switches too rapidly, more time is spent on switching and less time is spent on executing, leading to slow execution times. If the kernel switches too slowly, concurrent processes become choppy or slow.

### Process Lifecycle
When a process starts, the kernel places it on the ready queue. When the last process's time is up, the scheduler takes the next process, and runs a thread of that process: Load the thread's state (register, PC, stack pointer values) into CPU and load the environment (process's memory space). Now the process is _executing_ (since a thread of the process is executing). A running process has three possible outcomes: _exit_ (the process terminates), _interrupt_ (e.g., throwing an exception, keydown, keyup, etc; the scheduler places the process back on to the ready queue), or _waiting_ (the browser's renderer waiting for incoming data; the scheduler places the process on the waiting queue). If the anticipated event occurs, the waiting process is put back on to the ready queue. Notice that an interrupt doesn't just halt the CPU. Interrupts, like all events, must wait in line.

The ready and waiting queues aren't the only queues. There are USB queues, Disk queues, Ethernet queues, WiFi queues, Bluetooth queues, etc. These interface-specific queues are usually handled by _device drivers_. The device drivers then interact with the OS's scheduler.

~remark~. The data structure used to implement the queues depends on the OS's scheduling policy. Common implementations are singly- and doubly-linked lists.

There's something very important to keep in mind about all of this: _Both the OS and threads run on the same CPU._ Thus, when a given thread is running, the OS isn't running. And when the OS is running, no threads are running. Remember that the OS is a program like everything else (granted, a very special kind of program).

~question~. How does the OS—or more generally, a dispatcher—get control back?

First, differentiate between two kinds of events: __*Internal events*__ (events where a thread returns control voluntarily) and __*external events*__ (events where a thread is preempted; control is taken).

~example~. A thread requesting I/O for a file on disk is an internal event. The thread yields to the disk processes. Waiting on a signal and executing `yield` are all internal events. The `yield` function provided by C tells the scheduler that it will voluntarily give up the CPU if need be (useful for operations that may take hours or days to run).