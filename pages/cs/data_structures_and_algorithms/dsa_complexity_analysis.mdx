import { Plot } from "../../../components/hago";
import { Col } from "../../../components/Col";

<Head>
	<title>Asymptotic Analysis</title>
	<meta name={`description`} content={`Notes on complexity analysis.`}/>
</Head>

# Complexity Analysis
This section provides a general overview of algorithmic analysis in programming terms. We present the motivating question, its answer, as well as illustrations of the answer applied. The materials that follow assume a basic understanding of functions, limits, sequences, and series (i.e., comfort with ideas from a basic calculus and discrete mathematics course). For a thorough mathematical justification of algorithmic analysis,

1. [Knuth's Model](#knuths-model)
2. [Complexity Functions](#complexity-functions)
3. [Asymptotic Notations](#asymptotic-notations)
	1. [Limit-based Definitions](#limit-based-definitions)
		1. [Asymptotic Similarity](#asymptotic-similarity)
		2. [Asymptotic Equivalence](#asymptotic-equivalence)
		3. [Vindogrov Notation](#vindogrov-notation)
		4. [Order of Magnitude Estimate](#order-of-magnitude-estimate)
		5. [Limitations of Limit-based Notations](#limitations-of-limit-based-notations)
	2. [Non-limit-based Definitions](#non-limit-based-definitions)
		1. [Function Inequality](#function-inequality)
		2. [Big O Notation](#big-o-notation)
		3. [Big Omega Notation](#big-omega-notation)
		4. [Big Theta Notation](#big-theta-notation)
		5. [Little O Notation](#little-o-notation)
		6. [Little Omega Notation](#little-omega-notation)
4. [Relationships Between Asymptotic Notations](#relationships-between-asymptotic-notations)
5. [Common Time Complexities](#common-time-complexities)
	1. [Constant Order: ${\bigO{1}}$](#constant-order-bigo1)
	2. [Logarithmic Orders: ${\bigO{\log{n}}}$](#logarithmic-orders-bigologn)
	3. [Linear Orders: ${\bigO{n}}$](#linear-orders-bigon)
	4. [Multilogarithmic Orders: ${\bigO{k\log{n}}}$](#multilogarithmic-orders-bigoklogn)
	5. [Linearithmic Orders: ${\bigO{n \log{n}}}$](#linearithmic-orders-bigon-logn)
	6. [Quadratic Orders: ${\bigO{n^2}}$](#quadratic-orders-bigon2)
	7. [Exponential Orders: ${\bigO{2^n}}$](#exponential-orders-bigo2n)
	8. [Factorial Orders: ${\bigO{n!}}$](#factorial-orders-bigon)
6. [Common Problem Patterns](#common-problem-patterns)
7. [Limitations of Asymptotic Notations](#limitations-of-asymptotic-notations)

The most pertinent concern for analyzing algorithms is _efficiency_. One measure of efficiency is the amount of _time_ an algorithm takes to execute. That is, how long it takes given algorithm to solve a particular problem. To do so, however, we need some agreed-upon method for quantifying "speed." We can't rely on timing from start to finish if we want to compare algorithms across different machines, since every machine is different. Solving some differential equation on a small mobile phone will be slower than running the same operation on a super computer. Although it may be useful to know how fast the search runs on just one device model, it's not the principal concern in the analysis of algorithms. We want generality.

We often also want to know the amount of _space_ an algorithm takes to execute. For example, one algorithm ${A}$ might be faster than algorithm ${B}$ but requires a significant amount of memory. Determining space complexity can serve as tie breakers in choosing one algorithm over another. Perhaps algorithms ${C}$ and ${D}$ take the same amount of time, but ${C}$ consumes less memory. So how do we determine the amount of time or space an algorithm takes? One way is to count the number of _basic steps_ performed as a function of ${n,}$ where ${n}$ is the input size. These basic steps are listed below:

|                     | ~syntax~                               |
| ------------------- | -------------------------------------- |
| declaration         | ${\textbf{init} ~ a}$                  |
| assignment          | ${a \gets b}$                          |
| comparisons         | ${a \le b, a \lt b, a \ge b, a \gt b}$ |
| addition            | ${a + b}$                              |
| subtraction         | ${a - b}$                              |
| multiplication      | ${a \by b}$                            |
| real division       | ${\dfrac{a}{b}}$                       |
| integer division    | ${a / b}$                              |
| remainder           | ${a \rem b}$                           |
| bitwise-not         | ${\bnot x}$                            |
| bitwise-and         | ${a \band b}$                          |
| bitwise-or          | ${a \bor b}$                           |
| bitwise-xor         | ${a \bxor b}$                          |
| bitwise-left-shift  | ${a \bls b}$                           |
| bitwise-right-shift | ${a \brs b}$                           |
| goto                | ${\goto{x}}$                           |
| return              | ${\textbf{return}~x}$                  |

~example~. Consider the summation algorithm below, which computes ${\tsum{0 \le i \lt n}{}{i}.}$

<Algo>

1. ${\let{sum}{0}}$
2. __for__ (${\let{i}{0}}$, ${i \lt n}$, ${\df{increment}~i}$)
		1. ${\let{sum}{sum' + i}}$
3. __return__ ${sum}$

</Algo>

Counting the number of operations:

| ~operation~             | ~count~   | ~remark~                                                                        |
| ----------------------- | --------- | ------------------------------------------------------------------------------- |
| ${\let{sum}{0}}$        | ${1}$     | Performed exactly once.                                                         |
| ${\let{i}{0}}$          | ${1}$     | Performed exactly once.                                                         |
| ${i \lt n}$             | ${n + 1}$ | Comparison: performed after each iterate. It's true ${n}$ times and false once. |
| ${{sum' + i}}$          | ${n}$     | Addition: performed only if the guard returns true.                             |
| ${\let{sum}{sum' + i}}$ | ${n}$     | Assignment operation: performed only if the guared returns true.                |
| ${\df{increment}~{i}}$  | ${n}$     | Increment: Performed only if the guard returns true.                            |
| ${\text{bf}~{sum}}$     | ${1}$     | Performed exactly once.                                                         |

Summing the number of executions for each operation, denoted ${t(n),}$ we get:

$$
	\eqs{
		t(n) &= 1 + 1 + (n+1) + n + n + n + 1 \\
		&= 2 + (n+1) + 3n + 1 \\
		&= 3 + (n+1) + 3n \\ 
		&= 3 + n + 1 + 3n  \\
		&= 3 + 1 + n + 3n \\
		&= 4 + 4n \\
		&= 4(1+n) \\
		&= 4(n+1) \\
	}
$$

The final expression gives us the algorithm's runtime function as a closed form expression: ${t(n) = 4(n+1).}$ This is still just an estimate. If we wanted to be even more accurate, we would count the number of ~jump~ instructions executed by loop. And if we wanted to be even more accurate, we would count the number of calls to RAM the CPU made to retrieve data not found in its registers. And if we wanted to be even more accurate, we would count the number of steps the CPU took to check its processor registers, the number of steps needed to run through the cache hierarchy, and so on. The point is, counting steps will always be an estimation at best. This doesn't mean that step counting is a useless exercise. In fact, we'll often use step counting to build a base case or to build some intuition. Our final description, however, will be a _complexity function_. Before we ever analyze an algorithm, however, there's something else we must do — prove that the algorithm exists in the first place.

## Knuth's Model
Modern analysis of algorithms is based on Donald Knuth's work, found throughout his (gargantuan) series of ~the art of computer programming~. In sum, we  analyze algorithms by running through the checklist below, _in order_:

1. ${\ws}$ The algorithm is exists.
2. ${\ws}$ List of variables ${u}$ representing basic operations.
3. ${\ws}$ List of costs of each basic operation.
4. ${\ws}$ List of best, average, and worst case inputs.
5. ${\ws}$ Analyze execution frequency of the variables ${u.}$
6. ${\ws}$ Calculate total running time for each of the cases:
	$$
		\dsum{q}{} \tx{frequency}(u) \times \tx{cost}(u).
	$$

## Complexity Functions
To understand complexity analysis notations, we must understand how
functions grow. In the preceding examples, we stated no requirements about the functions we used to model efficiency. At this point, we've seen enough to handle formality.

> __~complexity function~.__ Let ${f}$ be a function from ${\reals}$ to ${\reals}$ with ${x \in \reals.}$ Then ${f}$ is a _complexity function_ if, and only if, the following axioms are true: (1) There exists a number ${n \in \reals^+}$ such that, if ${x \ge n,}$ then ${f(x) \in \reals.}$ (2) There exists a number ${m \in \reals^+}$ such that, if ${x \ge m,}$ then ${f(x) \ge 0.}$

Complexity analysis — for our purposes — is limited to the class of functions above. We do so for several reasons. First, resources like time and memory are easiest to interpret in terms of positive numbers. Second, we could make things even easier by just limiting ourselves to the natural numbers, but then we'd exclude swathe of useful functions — ${f(n)=\lg n,}$ ${f(n)=\sqrt n,}$ and even the sinusoids ${f(n)=\cos(n), f(n)=\sin(n), etc.}$ So, we restrict complexity analysis to real-valued functions.

Requirement (1) in the definition ensures that we won't run into functions that never get defined on the reals when given positive arguments. For example, ${f(n) = \sqrt{-n}.}$ Requirement (2) is similar in spirit; we don't want to work with functions that never become positive. It's perfectly fine for the function to return negative values (e.g., ${f(n) = \lg n}$), but it's not ok for the function to _always_ return negative values (e.g., ${f(n) = -n^2}$). 

## Asymptotic Notations
We begin with a few asymptotic notations based on limits.
### Limit-based Definitions
#### Asymptotic Similarity
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \approx g ~~\tx{as}~~ x \to \infty
> $$
> 
> if, and only if, there exists a constant ${c \neq 0}$ such that
>
> $$
>   \ll{x}{\infty}\dfrac{f(x)}{g(x)} = c.
> $$
> 
> If ${f \llt g,}$ we say that ${f}$ is _asymptotically similar_ to ${g.}$

#### Asymptotic Equivalence
> __~definition~.__ Let ${f}$ and ${g}$ be functions in ${\reals.}$ We write
> $$
>   f \sim g ~~\tx{as}~~ x \to \infty
> $$
>
> if, and only if,
> 
> $$
>   \ll{x}{\infty} \dfrac{f(x)}{g(x)} = 1.
> $$
> 
> If ${f \sim g,}$ we say that ${f}$ and ${g}$ are _asymptotically equivalent_.

#### Vindogrov Notation
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \llt g ~~\tx{as}~~ x \to \infty
> $$
> 
> if, and only if,
>
> $$
>   \ll{x}{\infty}\dfrac{f(x)}{g(x)} = 0.
> $$
> 
> If ${f \llt g,}$ we say that ${f}$ is _asymptotically smaller_ than ${g.}$

#### Order of Magnitude Estimate
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \asymp g ~~\tx{as}~~ x \to \infty
> $$
> 
> if, and only if,
>
> $$
>   \ll{x}{\infty}\dfrac{f(x)}{g(x)} = 0 \\[1em]
>   \tx{and} \\[1em]
>   \ll{x}{\infty}\dfrac{g(x)}{f(x)} = 0 \\
> $$
> 
> If ${f \asymp g,}$ we say that ${f}$ is _asymptotically equal to_ ${g.}$


#### Limitations of Limit-based Notations
The problem with limit based notations is that they can only handle functions that actually converge. They can't address divergent functions, or functions that diverge at certain intervals. Because of this problem, we might want to tailor the function's domain to avoid these "glitches." But in doing so, we risk hiding details about asymptotic behavior that we probably _don't want_ to hide. In short, for some functions, limit-based notations put us in a bind. The only way out is to use weaker definitions.

### Non-limit-based Definitions 
#### Function Inequality
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> $$
>   f \le g
> $$
> iff ${f(x) \le g(x)}$ for all ${x \in A.}$

~example~. Suppose ${A = \set{0,1,2,3}}$ and ${B=\reals.}$ Let ${f(x) = x + 1,}$ and let ${g(x) = x + 5.}$ Examining the mappings:

| ${x}$ | ${f(x)}$  | ${g(x)}$  | ${f(x) \le g(x)}$ |
| ----- | --------- | --------- | ----------------- |
| 0     | ${0+1=1}$ | ${0+5=5}$ | true              |
| 2     | ${2+1=3}$ | ${1+5=6}$ | true              |
| 1     | ${1+1=2}$ | ${2+5=7}$ | true              |
| 3     | ${3+1=4}$ | ${3+5=8}$ | true              |

Thus, we can say that ${f \le g,}$ since ${f(x) \le g(x)}$ for every ${x}$ in the domain ${A.}$ There are a few important points to keep in mind here. We can _never_ compare two functions with different domains per the definition.

> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> $$
>   f \le g ~~\tx{as}~~ x \to \infty
> $$
> iff there exists a constant ${a \in A}$ such that, for all ${x \gt a,}$ it follows that ${f(x) \le g(x).}$

This definition is similar to the last, except we're now making the statement that ${f}$ is less than or equal to ${g}$ for sufficiently large arguments ${x.}$ And again, we can only compare two functions if they have the same domain. The domain could be a finite set, a countably infinite set like ${\nat,}$ or an uncountably infinite set like ${\reals.}$


#### Big O Notation
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \in \bigO{g} ~~\tx{as}~~ x \to \infty
> $$
> 
> if, and only if, there exists a positive constant ${c \in A}$ and a constant ${a \in A}$ such that
> $$
>   x \gt a \nc \abs{f(x)} \le c \by {g(x)}
> $$

This definition is simply an extension of the last, except it's a much stronger claim. We're saying that ${f(x) \le g(x)}$ is true for sufficiently large ${x,}$ _and_ it's true if multiply ${g(x)}$ by some constant ${c.}$

#### Big Omega Notation
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \in \bigOmega{g} ~~\tx{as}~~ x \to \infty
> $$
> 
> if, and only if, there exists a positive constant ${c \in A}$ and a constant ${a \in A}$ such that
> $$
>   x \gt a \nc \abs{f(x)} \ge c \by {g(x)}
> $$

The definition here is similar to the last, except we've now switched the direction of the inequality. With big O, we're saying that ${f}$ grows no faster than ${g,}$ and with big omega, we're saying that ${f}$ grows _at least as fast_ as ${g.}$

#### Big Theta Notation
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \in \bigTheta{g} ~~\tx{as}~~ x \to \infty
> $$
> 
> if, and only if, there exists positive constants ${c_0,c_1 \in A}$ and an ${a \in A}$ such that
> $$
>   x \gt a \nc c_0 \by g(x) \le \abs{f(x)} \le c_1 \by g(x).
> $$

When we write ${f \in \bigTheta{g},}$ we're saying that ${f}$ grows at the same rate as ${g.}$ Notice that ${f \in \bigTheta{g}}$ implies that ${f \in \bigOmega{g}}$ and ${f \in \bigO{g}.}$

#### Little O Notation
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \in o(g) ~~\tx{as}~~ x \to \infty
> $$
> 
> iff _for all positive_  ${c \in A,}$ there exists an ${a \in A}$ such that
>
> $$
>   x \gt a \nc \abs{f(x)} \le c \by {g(x)}
> $$

This definition looks like big O, but it's very different. If we say that ${f \in o(g),}$ we're stating that ${f}$ _grows slower_ than ${g.}$ This also looks like Vindogrov notation, but it's weaker — we aren't requiring ${f/g}$ to converge, which is a much harder condition to satisfy.

#### Little Omega Notation
> __~definition~.__ Let ${f}$ and ${g}$ be functions from ${A}$ to ${B}$ with ${x \in A.}$ We write
> 
> $$
>   f \in \omega(g) ~~\tx{as}~~ x \to \infty
> $$
> 
> iff for all positive ${c \in A,}$ there exists an ${a \in A}$ such that
>
> $$
>   x \gt a \nc \abs{f(x)} \ge c \by {g(x)}
> $$

This definition looks like little O, but it's a stronger claim. Claiming that ${f \in \omega(g)}$ is equivalent to claiming ${f}$ _grows faster_ than ${g.}$

## Relationships Between Asymptotic Notations  
Because the non-limit-based definitions are weaker than the limit-based definitions, satisfying the limit-based definitions will satisfy the non-limit-based definitions. Similarly, because some of the non-limit based definitions are either stronger or weaker than the others, satisfying one can imply another. This leads to a variety of logical implications:

<Grid cols={3}>

> ${f \in \bigO{g}}$ and ${f \in \bigOmega{g} \nc f \in \bigTheta{g}}$

> ${f \in \bigTheta{g} \nc f \in \bigO{g}}$ and ${f \in \bigOmega{g}}$

> ${f \in \bigO{g} \nc g \in \bigOmega{f}}$

> ${f \in \bigOmega{g} \nc g \in \bigO{f}}$

> ${f \in o(g) \nc g \in \omega(f)}$

> ${f \in \omega(g) \nc g \in o(f)}$

> ${f \in o(g) \nc f \in \bigO{g}}$

> ${f \in \omega(g) \nc f \in \bigOmega{g}}$

> ${f \sim g \nc f \in \bigTheta{g}}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)}=1 \nc f \sim g}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)}=0 \nc f \llt g}$

> ${f \llt g}$ and ${g \llt f \nc f \asymp g}$

> ${f \asymp g \nc f \in \bigTheta{g}}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)} \neq 0 \nc f \approx g}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)}}$ diverges ${\nc f \in \omega(g)}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)} \neq 0 \nc f \approx g}$

> ${f \approx g \nc f \in \bigOmega{g}}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)} = 0 \nc f \in o(g)}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)} = c \nc f \in \bigO{g}}$

> ${\ll{x}{\infty}\dfrac{f(x)}{g(x)} = c}$ and ${c \neq 0 \nc f \in \bigTheta{g}}$

</Grid>

The relationships above are particularly useful when we recall L'Hospital's Rule:

> If ${\ll{x}{\infty}f(x)}$ diverges and ${\ll{x}{\infty}g(x)}$ diverges, then ${\ll{x}{\infty}\dfrac{f(x)}{g(x)} = \ll{x}{\infty}\dfrac{f'(x)}{g'(x)}.}$

As well as the various other limit theorems:

<Grid cols={2}>

> ${\ll{x}{a}C = C,}$ where ${C}$ is a constant
 
> ${\ll{x}{a}C \by f(x) = C \by \ll{x}{a} f(x).}$

> ${\ll{x}{a}(f(x) + g(x)) = {\ll{x}{a}f(x)} + {\ll{x}{a}g(x)}}$

> ${\ll{x}{a}(f(x) - g(x)) = \ll{x}{a}f(x) - \ll{x}{a}g(x)}$

> ${\ll{x}{a}f(x)g(x) = {\ll{x}{a}f(x)} \by {\ll{x}{a}g(x)}}$

> ${\ll{x}{a}\dfrac{f(x)}{g(x)}=\dfrac{\ll{x}{a}f(x)}{\ll{x}{a}g(x)}}$ where ${\ll{x}{a}g(x) \neq 0}$

> ${\ll{x}{a}f(x)^n = \ar{\ll{x}{a}f(x)}^n}$ where ${n \in \pint}$

> ${\ll{x}{a}\sqrt[n]{f(x)}=\sqrt[n]{\ll{x}{a}{f(x)}}}$ where ${n \in \pint}$

> ${\ll{x}{0} \dfrac{\sin{x}}{x} = 1.}$

> ${\ll{x}{a} \dfrac{1}{x} = 0.}$

> ${\ll{x}{a} P(x) = Q(x)}$ where ${P(x)}$ and ${Q(x)}$ are polynomials

> ${\ll{x}{a}\dfrac{f(x)}{g(x)} = \dfrac{P(x)}{Q(x)}}$ where ${P(x)}$ and ${Q(x)}$ are polynomials

> If ${\ll{x}{a}g(x)=\ll{x}{a}h(x)=L}$ and ${g \le f \le h}$ as ${x \to a,}$ then ${\ll{x}{\infty}f(x)=L}$

> ${\ll{x}{c}f(x) = L \iff \ll{x}{c^+}f(x) \tx{ and } \ll{x}{c^-}f(x)}$

</Grid>

## Common Time Complexities
Some common time complexities are presented below.

### Constant Order: ${\bigO{1}}$

<Grid cols={2}>

These operations are assumed to be instantaneous. Common examples include hashtable lookups (assuming a good implementation), array read/write, pushing and popping elements from a stack, and arithmetic operations.

<Plot data={[{f:(x)=>(1)}]} domain={[0,10]} range={[0,10]} ticks={0}/>

</Grid>

### Logarithmic Orders: ${\bigO{\log{n}}}$

<Grid cols={2}>

Logarithmic operations grow faster than constant functions, but still _very slowly_. Examples: Binary search, balanced binary search tree lookup, processing number digits.

<Plot data={[{f:(x)=>(Math.log(x))}]} domain={[0,10]} range={[0,10]} ticks={0}/>

</Grid>

### Linear Orders: ${\bigO{n}}$

<Grid cols={2}>

Linear functions grow faster than logarithmic functions. Generally, linear functions are associated with iterating through a linear structure. Examples: Linear search, two-pointer algorithms, some types of greedy algorithms, tree and graph traversals, popping/pushing through an entire stack, enqueuing/dequeuing through an entire queue.

<Plot data={[{f:(x)=>(x)}]} domain={[0,10]} range={[0,10]} ticks={0}/>

</Grid>

### Multilogarithmic Orders: ${\bigO{k\log{n}}}$

<Grid cols={2}>

Where ${k \gt 0}$ is a constant, multilogarithmic functions are constant multiples of logarithmic functions. Accordingly, they run slower than linear functions. Examples: Inserting/removing heap nodes ${k}$ times, binary search ${k}$ times, etc. Generally, multilogarithmic functions are associated with algorithms tasked with searching for the first ${k}$ matches.

<Plot data={[{f:(x)=>(3*Math.log(x))}]}domain={[0,10]}range={[0,10]}ticks={0}/>

</Grid>

### Linearithmic Orders: ${\bigO{n \log{n}}}$

<Grid cols={2}>

In contrast to multilogarithmic operations, linearithmic operations are logarithmic multiples of a linear function. Classic examples include: Comparison-based sorting (e.g., standard merge sort), divide-and-conquer algorithms.

<Plot data={[{f:(x)=>(x*Math.log(x))}]}domain={[0,10]}range={[0,10]}ticks={0}/>

</Grid>


### Quadratic Orders: ${\bigO{n^2}}$

<Grid cols={2}>

This is a common runtime order. Examples: Nested loops (e.g., visiting every matrix entry) and brute force solutions. For ${n \lt 1000,}$ this order isn't so bad on modern computers. Anything beyond ${1000}$ and we're in questionable territory.

<Plot data={[{f:(x)=>(x**2)}]}domain={[0,10]}range={[0,10]}ticks={0}/>

</Grid>


### Exponential Orders: ${\bigO{2^n}}$

<Grid cols={2}>

Exponential orders grow very rapidly, and almost always require some form of memoization to avoid repeated computations. Algorithms with exponential order runtimes are commonly associated with combinatorial problems and backtracking. Examples include the recursive Fibonacci algorithm. 

<Plot data={[{f:(x)=>(2**x)}]}domain={[0,10]}range={[0,10]}ticks={0}/>

</Grid>

### Factorial Orders: ${\bigO{n!}}$
Factorial orders are extreme; so much so that we've omitted the graph here. Examples: Generating permutations and some backtracking algorithms.



## Common Problem Patterns
Certain problems are so commonly associated with certain algorithms that there are keyword terms we want to watch out for. These terms are presented below.

## Limitations of Asymptotic Notations
Big-O notation is _never_ a predictor of algorithmic performance. This is an elementary mathematical fact that's often overlooked when analyzing algorithms. All that asymptotic notations provide are _bounds_, and nowhere in the mathematical literature would we find an assertion that a bound is equivalent to a future state. In fact, if we turned to how asymptotic notations are used in mathematics, we'd find that they're used largely as methods of "short-handing" expressions. For example, the standard proof of the general power rule for derivatives involves binomial terms, leading to the general result:

$$
\begin{aligned}
	\lim\limits_{\Delta x \to 0} \dfrac{\Delta f}{\Delta x} &= \lim\limits_{\Delta x \to 0} \left(nx^{n-1} + \dfrac{\bigO{(\Delta x)^2}}{\Delta x}\right) \\
	&= nx^{n-1} + 0 \\
	&= nx^{n-1}
\end{aligned}
$$