import { Plot } from "../../../components/illus/components/Plot/Plot";
import { Plot as HPlot } from "../../../components/hago";
import { Col } from "../../../components/Col";

<head>
	<title>Complexity Analysis</title>
	<description>Notes on complexity analysis.</description>
</head>

# Complexity Analysis

<div className={"outline"}>

1. [Cases](#cases)
2. [Big-O Notation](#big-o-notation)
3. [Big-omega Notation](#big-omega-notation)
4. [Big Theta Notation](#big-theta-notation)

</div>

This section provides a general overview of algorithmic analysis in
programming terms. We present the motivating question, its answer, as well
as illustrations of the answer applied. The materials that follow assume a
basic understanding of functions, limits, sequences, and series (i.e.,
comfort with ideas from a basic calculus and discrete mathematics course).
For a thorough mathematical justification of algorithmic analysis,

One concern for analyzing algorithms is _efficiency_. While this is the
most common concern, it is second to another—_correctness_. If an algorithm
is incorrect, its efficiency is unimportant.[^efficiency_note] In these
materials, we suppose that we've gotten over the correctness hurdle, and
focus on efficiency.

[^efficiency_note]:

For the most part. In later sections, we will see how analyzing the
efficiency of an incorrect algorithm can lead to insight elsewhere.

The most common procedure for analyzing algorithms is determining the
amount of _time_ an algorithm takes to execute. In essence, how long it
takes a given algorithm to solve a particular problem. To do so, however,
we need some agreed-upon method for quantifying time. We can't rely on
units like seconds, because every machine is different. Solving some
differential equation on a small mobile phone will be slower than running
the same search on a super computer. Although it may be useful to know how
fast the search runs on one device over another, it's not the principal
concern in the analysis of algorithms. The search algorithm is the same for
both the small mobile and the super computer. What we want to know is how
the algorithm compares to another search algorithm, _regardless of
device_.[^complexitynote]

[^complexitynote]:

Time complexity is the way we measure how much time an algorithm takes to
execute (the algorithm's _runtime_) according to its input size.

Occasionally, analyzing algorithms requires determining the amount of
_space_ an algorithm takes to execute. For example, one algorithm ${A}$
might be faster than algorithm ${B}$ but requires a significant amount of
memory. Determining space complexity can often be immensely. Perhaps
algorithms ${C}$ and ${D}$ take the same amount of time, but ${C}$ consumes
less memory.

So how do we determine the amount of time or space an algorithm takes? The
first step is to count the number of _fundamental operations_ performed as
a function of ${n,}$ where ${n}$ is the number of input values. In the
pseudocode below, each line in the program on the left presents a unique
operation. On the right are the time complexities for each line, along with
comments (click on the comment to expand, or program line to see the
accompanying comment).

We list the operations as exhaustively as we can because in these first few
sections we will count all of the operations. However, as we progress, we
will learn that we rarely count every operation. Some operations are
counted, others ignored. That said, let's consider an example. Suppose we
had the following algorithm:

```c
int sum = 0;
 for (int i = 0; i < n; i++):
 sum = sum + i;
 return sum;
```

Counting the number of operations:

| operation      | count                                                                                                                                                         |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `int sum = 0;` | ${1.}$                                                                                                                                                        |
| `int i = 0;`   | ${1.}$                                                                                                                                                        |
| `i < n;`       | ${n + 1.}$ The comparison is performed after each execution of the loop's body. Thus, it is true ${n}$ times and false once.                                  |
| `sum + i;`     | ${n.}$ The addition operation is performed only if the test condition (the preceding comparison operation) returns `true`. Thus, it is performed ${n}$ times. |
| `sum = sum;`   | ${n.}$ The assignment operation is performed only if the test condition returns `true`. Thus, it is performed ${n}$ times.                                    |
| `i++;`         | ${n.}$ The increment is performed only if the test condition returns `true`. Thus, it is performed ${n}$ times.                                               |
| `return sum;`  | ${1.}$ The `return` operation is performed exactly once.                                                                                                      |

Summing the number of executions for each operation, ${\texttt{T}(n),}$ we
get:

$$
 \begin{aligned} \texttt{T}(n) &= 1 + 1 + (n+1) + n + n + n \\ &= 1 + 1
 + n + 1 + n + n + n \\ &= 1 + 1 + 1 + n + n + n + n \\ &= 3 + 4n \end
 {aligned}
$$

The final expression gives us a function of the algorithm's total run time:
${\texttt{T}(n) = 4n + 3.}$ When we conduct complexity analysis, we want to
express this runtime in terms of Big-O notation. To do so, we want to:

1. Remove the constants.
2. Remove the lowest terms.

In this case, the constant is ${5,}$ and the lowest term is ${4.}$ Dropping
the constant and the lowest term, we have some function ${g(n) = n,}$ and
we express this as ${O(n).}$ We call this __linear time__, and we say that
"The algorithm above runs no faster than ${n,}$" or "The runtime complexity
is ${O(n).}$" Now, compare the algorithm above with the following:

```c
return (n * (n + 1)) / 2
```

Counting the number of operations:

| operation           | count |
| ------------------- | ----- |
| `n + 1`             | 1     |
| `n * result(n + 1)` | 1     |
| `result(n + 1) / 2` | 1     |
| `return result`     | 1     |

Summing the number of executions:

$$
 \begin{aligned} \texttt{T}(n) &= 1 + 1 + 1 + 1\\[1em] &= 4 \end {aligned}
$$

This algorithm has a running time function of ${\texttt{T}(n) = 4.}$
Whenever the running time function is constant, we say that the algorithm
runs on __constant time__, and we denote this with ${O(1).}$ This is a
concise way of saying that algorithm's runtime does not depend on the input
size ${n.}$ We can throw as many ${n}$ as we would like at the algorithm,
and the runtime will never change. It's constant.

Compare these two algorithms. Clearly, the first algorithm is slower than
the second. We have function ${g(n) = n,}$ and ${g(n) = 1.}$ If we plot
these two functions:

![Linear versus constant plot. One is a flat, horizontal line, the other is angled.](https://res.cloudinary.com/sublimis/image/upload/v1652743076/cs/linear_versus_constant_plot.svg)

we see that ${g(n)}$ will always grow faster than ${g(1).}$ This indicates
that ${g(1)}$ will always be faster than ${g(n)}$—${g(1)}$ never grows to
begin with.

## Cases

How well an algorithm performs depends on its inputs. Consider the simple
linear search:

```c
bool LinearSearch(TYPE[] array, TYPE key):
 int n = array.Length();
 for (int i = 0; i < n; i++):
 if (arr[i] == key): return true;
 return false;
```

If the key is found at `arr[0]`, the algorithm executes in just one
iteration. If the key is found at `arr[n-1]`, the algorithm executes in `n`
iterations (with `n` evaluations of the loop's condition). If the key isn't
found anywhere in the array, the algorithm executes in `n` iterations, with
`n+1` evaluations of the loop's condition. Thus, there are three unique
scenarios:

1. Key is found after just one iteration.
2. Key is found after ${n}$ iterations. - Key is not found.

Because of this phenomenon, algorithmic analysis can be performed in three
different ways:

- The __best-case time__ of an algorithm is the algorithm's _smallest
  possible run-time_, over all possible fixed-size inputs. The algorithmic
  analysis for determining an algorithm's best-case time is called the
  _best-case analysis_. When someone gives us an algorithm's best-case time
  ${t,}$ they're essentially telling us that the algorithm has a speed
  limit ${t}$—it can't get any faster than ${t.}$

- The __worst-case time__ of an algorithm is the algorithm's _largest
  possible run-time_, over all possible fixed-size inputs. The algorithmic
  analysis for determining an algorithm's best-case time is called the
  _worst-case analysis_. When we're given an algorithm's worst-case time
  ${w,}$ we're being told, "This algorithm can't get any slower than
  ${w.}$"

- The __average-case time__ of an algorithm is the algorithm's _average
  possible run-time_, over all possible fixed-size inputs. The algorithmic
  analysis for determining an algorithm's best-case time is called the
  _average-case analysis_. When we're given an algorithm's average-case
  time ${a}$, we're being told, "On average, the algorithm has run time of
  ${a.}$"

## Growth of Functions

To understand complexity analysis notations, we have to understand how
functions grow. First, suppose we have two distinct functions of real
numbers: ${f(x)}$ and ${g(x).}$ As distinct real-number functions, we can
likely make a lot of different claims about the _relationship_ between
these two functions. In the following sections, we focus on relationships
based on _growth_.

To put things in context, let's be explicit about the possible growth
relationships between ${f}$ and ${g:}$

### Asymptotic Smallness

One claim we can make about ${f(x)}$ and ${g(x)}$ is the following:

> "${f(x)}$ is asymptotically smaller than ${g(x).}$"

The claim above can be notated as:

$$
 f(x) \ll g(x)
$$

For ${f(x)}$ to actually be asymptotically smaller than ${g(x),}$ the
following definition must be satisfied:

> __definition__. ${f(x) \ll g(x)}$ if and only if:
>
> $$
>  \lim\limits_{x \to \infty} \dfrac{f(x)}{g(x)} = 0
> $$

In other words, as the 𝑥 inputs to ${f(x)}$ and ${g(x)}$ grow very large,
${g(x)}$ will eventually "outrun" ${f(x).}$

## Big-O Notation

Here's the definition of Big-O notation:

> __definition__. ${f(n)}$ is ${O(g(n))}$ if there exists a constant C and
> a real number ${n_0,}$ such that for all ${n > n_0,}$
>
> $$
>  f(n) \leq C \cdot g(n)
> $$

Let's consider an example. Suppose we have the function:

$$
 f(n) = 4n^2 + 16n + 2
$$

Question: Is ${f(n)}$ ${O(n^4)}$? Well, let's go back to our definition.
The condition we want to verify is:

$$
 f(n) \leq C \cdot g(n)
$$

In our example, ${g(n)}$ is defined as ${n^4.}$ So, we can reframe the
question as:

> Is there a constant ${C}$ and some real number ${n_0}$ such that, for all
> ${n > n_0,}$
>
> $$
>  4n^2 + 16n + 2 < C \cdot n^4~~?
> $$

The value ${n_0}$ is essentially a starting point. We want to find a
constant ${C}$ and a starting point ${n_0}$ such for all the ${n}$s after
${n_0,}$ the condition above is true.

We can start by trying out different values for ${C}$ and ${n_0.}$

| ${n}$ | ${4n^2 + 16n + 2}$ | ${n^4}$ | ${4n^2 + 16n + 2 \leq n^4}$ |
| :---: | :----------------: | :-----: | :-------------------------: |
|   0   |         2          |    0    |            false            |
|   1   |         22         |    1    |            false            |
|   2   |         50         |   16    |            false            |
|   3   |         86         |   81    |            false            |
|   4   |        130         |   256   |            true             |
|   5   |        182         |   625   |            true             |
|   6   |        242         |  1296   |            true             |
|   7   |        310         |  2401   |            true             |
|   8   |        386         |  4096   |            true             |
|   9   |        470         |  6561   |            true             |
|  10   |        562         |  10000  |            true             |

Examining the table above, it looks like after ${n = 4,}$ our condition
${f(n) \leq C \cdot g(n),}$ where ${C=1,}$ is true. If we graphed the
functions above, we would get:

<Plot
 functions={[
  { f: (x) => 4 *x ** 2 + 16 * x + 2, color: "var(--red)" },
{ f: (x) => x** 4, color: "var(--blue)" },
 ]}
 geo={[
  {
   type: "label",
   id: "f(n) = 4n^2 + 16n + 2",
   xy: [-17, -5],
   fill: "var(--red)",
   w: 100,
  },
  { type: "label", id: "g(n) = n^4", xy: [5, 5], fill: "var(--blue)" },
 ]}
 scale={60}
 xLabel={"n"}
 id={"complexity_plot_1"}
 domain={[-20, 20]}
/>

Notice how much faster ${f(n)}$ grows compared to ${g(n).}$ One way to
think about big-O notation: When we make the statement:

> "${f}$ is big-O of ${g}$"

we're implying that there's some point along the 𝑥-axis where, after that
point, the output of some stretched version ${g}$ will **always be
greater** than ${f.}$ Alternatively, we can think of big-O notation as a
nicer way of expressing the idea that some function ${f}$ is a subset of
some function ${C \cdot g:}$

<Plot
 geo={[
  {
   type: "rectangle",
   xy: [0, 0],
   w: 20,
   h: 12,
   class: "yellowRectangle",
  },
  { type: "circle", xy: [0, 0], r: 5, class: "blueCircle" },
  { type: "circle", xy: [-2, 0], r: 2, class: "plainCircle" },
  { type: "label", id: "f", xy: [-2.4, 0.5], fontSize: 1.3 },
  { type: "label", id: "O(g)", xy: [1, 2], fontSize: 1.3 },
 ]}
 marginTop={0}
 marginBottom={0}
 noAxes={true}
 scale={40}
 id={"big_o_subset"}
/>

Another way of thinking about big-O notation is through some basic real
analysis:

> If
> ${\lim\limits_{n \to \infty} \paren{\dfrac{f(n)}{g(n)}} = d < \infty,}$
> then ${f(n)}$ is ${O(g(n)).}$

For example, how do we know that ${\log_{2}(n) = O(2n)?}$ Well, we can
start by applying the limit:

$$
 \lim\limits_{n \to \infty} \paren{\dfrac{\log_{2} n}{2n}} = \dfrac{\infty}{\infty}
$$

Using L'Hoptial's rule:

$$
 \lim\limits_{n \to \infty} \paren{\dfrac{\log_{2} n}{2n}} = \lim\limits_{n \to \infty} \paren{\dfrac{1}{2n}}
$$

The right-hand limit goes to ${0.}$ Thus, it follows that
${0 = d < \infty,}$ and we can say that ${\log_{2}n = O(2n).}$

## Big-omega Notation

Here's the definition of big-omega:

> __definition__. A function ${f(n)}$ is ${\Omega(g(n))}$ iff there exists
> some constant ${C}$ and some number ${n_0}$ such that for all
> ${n > n_0,}$
>
> $$
>  f(n) \geq C \cdot g(n)
> $$

Big-omega's definition contains substantially the same wording as Big-O's
definition, with the key difference being it's last condition:

$$
 f(n) \geq C \cdot g(n)
$$

To illustrate, let's use the same example we saw earlier.
${f(n) = 4n^2 + 16n + 2.}$ Here's our question:

> Is ${f(n)}$ ${\Omega(n^2)}$?

Let's construct an output table:

| ${n}$ | ${4n^2 + 16n + 2}$ | ${n^2}$ | ${4n^2 + 16n + 2 \geq n^2}$ |
| :---: | :----------------: | :-----: | :-------------------------: |
|   0   |         2          |    0    |            true             |
|   1   |         22         |    1    |            true             |
|   2   |         50         |    4    |            true             |
|   3   |         86         |    9    |            true             |
|   4   |        130         |   16    |            true             |
|   5   |        182         |   25    |            true             |
|   6   |        242         |   36    |            true             |
|   7   |        310         |   49    |            true             |
|   8   |        386         |   64    |            true             |
|   9   |        470         |   81    |            true             |
|  10   |        562         |   100   |            true             |

Clearly, given ${C=1}$ and ${n}$ starting at ${0}$ (that is, ${n_0 = 0}$),
${4n^2 + 16n + 2}$ will always be greater ${n^2.}$ Examining the graphs:

<Plot
 functions={[
  { f: (x) => 4 *x ** 2 + 16 * x + 2, color: "var(--red)" },
{ f: (x) => x** 2, color: "var(--blue)" },
 ]}
 geo={[
  {
   type: "label",
   id: "f(n) = 4n^2 + 16n + 2",
   xy: [-17, -5],
   fill: "var(--red)",
   w: 100,
  },
  { type: "label", id: "g(n) = n^2", xy: [5, 5], fill: "var(--blue)" },
 ]}
 scale={60}
 xLabel={"n"}
 id={"complexity_plot_2"}
 domain={[-20, 20]}
/>

When we say:

> "${f}$ is big-omega of ${g.}$"

we're implying that there's some point along the 𝑥-axis where, after that
point, a, the output of some stretched version of ${g}$ **can never be
greater than** the output of ${f.}$ That is, no matter how hard ${g}$
tries, it will never return an output greater than ${f}$'s output.

<Plot
 geo={[
  {
   type: "rectangle",
   xy: [0, 0],
   w: 20,
   h: 12,
   class: "yellowRectangle",
  },
  { type: "circle", xy: [0, 0], r: 5, class: "blueCircle" },
  { type: "circle", xy: [-2, 0], r: 2, class: "plainCircle" },
  { type: "label", id: "\\Omega(g)", xy: [1, 2], fontSize: 1.5 },
  { type: "label", id: "f", xy: [-2.5, 0.5], fontSize: 1.3 },
 ]}
 marginTop={0}
 marginBottom={0}
 noAxes={true}
 scale={40}
 id={"big_omega_subset"}
/>

## Big-theta Notation

Here's the definition of big-theta notation:

> __definition__. ${f(n)}$ is ${\Theta(g(n))}$ iff:
>
> 1. ${f(n)}$ is ${O(g(n))}$, _and_
> 2. ${f(n)}$ is ${\Omega(g(n))}$

Notice the two necessary conditions in this definition. For some function
${f}$ to be ${\Theta(g),}$ it must be the case that ${f}$ is ${O(g)}$ and
${f}$ is ${\Omega(g).}$ Expanding the definition more explicitly:

> __definition__. ${f(n)}$ is ${\Theta(g(n))}$ iff there exists some
> constant ${C}$ and some number ${n_0}$ such that, for all ${n > n_0,}$
>
> 1. ${f(n) \geq C \cdot g(n),}$ _and_
> 2. ${f(n) \leq C \cdot g(n)}$

Let's illustrate with the same function we've used thus far:
${f(n) = 4n^2 + 16n + 2.}$ The question:

> Is ${f(n)}$ ${\Theta(n^2)}$?

Well, from our output table earlier, we know that ${f(n)}$ is
${\Omega(n^2).}$

| ${n}$ | ${4n^2 + 16n + 2}$ | ${n^2}$ | ${4n^2 + 16n + 2 \geq n^2}$ |
| :---: | :----------------: | :-----: | :-------------------------: |
|   0   |         2          |    0    |            true             |
|   1   |         22         |    1    |            true             |
|   2   |         50         |    4    |            true             |
|   3   |         86         |    9    |            true             |
|   4   |        130         |   16    |            true             |
|   5   |        182         |   25    |            true             |
|   6   |        242         |   36    |            true             |
|   7   |        310         |   49    |            true             |
|   8   |        386         |   64    |            true             |
|   9   |        470         |   81    |            true             |
|  10   |        562         |   100   |            true             |

But is ${f(n)}$ ${O(n^2)}$ as well? Sure, we can set ${C = 6:}$

| ${n}$ | ${4n^2 + 16n + 2}$ | ${6n^2}$ | ${4n^2 + 16n + 2 \leq 6n^2}$ |
| :---: | :----------------: | :------: | :--------------------------: |
|   0   |         2          |    0     |            false             |
|   1   |         22         |    6     |            false             |
|   2   |         50         |    24    |            false             |
|   3   |         86         |    54    |            false             |
|   4   |        130         |    96    |            false             |
|   5   |        182         |   150    |            false             |
|   6   |        242         |   216    |            false             |
|   7   |        310         |   294    |            false             |
|   8   |        386         |   384    |            false             |
|   9   |        470         |   486    |             true             |
|  10   |        562         |   600    |             true             |
|  11   |        662         |   726    |             true             |

With ${C=6,}$ we see that at the value ${n = 9,}$ we have
${f(n) \leq C \cdot g.}$ This proves that ${f(n)}$ is ${O(n^2)}$ — there is
in fact a ${C}$ (namely ${C=6}$) and an ${n_0}$ (${n_0 = 9}$), such that
${f \leq C \cdot g.}$

What does this mean from a set theory perspective? From the definition, we
see that big-theta notation communicates a stronger statement — if we
satisfy the definition of big-theta, it follows that we satisfy the
definitions of big-omega and big-O. This implies the following:

> _Proposition_. All functions that satisfy the definition of big-theta
> satisfy the definition of big-O, but not all functions that satify the
> definition of big-O satisfy the definition of big-theta.

The same goes for big-omega:

> _Proposition_. All functions that satisfy the definition of big-theta
> satisfy the definition of big-omega, but not all functions that satify
> the definition of big-omega satisfy the definition of big-theta.

From the propositions above, we can draw two further equivalent
propositions:

> _Proposition_. The set of functions that satisfy the definition of
> big-theta is a subset of the set of functions that satisfy the definition
> of big-O.

> _Proposition_. The set of functions that satisfy the definition of
> big-theta is a subset of the set of functions that satisfy the definition
> of big-omega.

> _Proposition_. The set of functions that satisfy the definition of
> big-theta is a subset of _both_:
>
> 1. the set of functions that satisfy the definition of big-O, and
> 2. the set of functions that satisfy the definition of big-omega

We can visualize these propositions as follows:

<Plot
 geo={[
  {
   type: "rectangle",
   xy: [0, 0],
   w: 20,
   h: 12,
   class: "yellowRectangle",
  },
  { type: "circle", xy: [-2, 0], r: 5, class: "blueCircle" },
  { type: "circle", xy: [2, 0], r: 5, class: "redCircle" },
  { type: "label", id: "O(g)", xy: [4.3, 5.5], fontSize: 1.3 },
  { type: "label", id: "\\Omega(g)", xy: [-6.3, 5.5], fontSize: 1.3 },
  { type: "label", id: "\\Theta(g)", xy: [-1.2, 3], fontSize: 1.3 },
 ]}
 marginTop={0}
 marginBottom={0}
 noAxes={true}
 scale={40}
 id={"big_theta_sets"}
/>

## Running Times

From a mathematical perspective, the asymptotic notations above aren't all
that interesting. In fact, they're generally used as tools for simplifying
algebraic manipulation. In computer science, however, they're immensely
useful for comparing algorithms.

For example, suppose our algorithm has some form like the following:

```c
function f(dataArray) {
 for (let i = 0; dataArray.length; i++) {
 // operations
 for (let j = 0; dataArray[i].length; j++) {
 // operations
 };
 };
 for (let k = 0; dataArray.length; k++) {
 // operations
 };
 // operation
 // operation
}
```

This function might have a running time of the form:

$$
 T(n) = 3n^2 + 8n + 2
$$

For simplicity, let's just say it's ${T(n) = n^2.}$ If we graphed this
function:

<HPlot
 data={[{ f: (x) => x ** 2, color: "var(--red)" }]}
 scale={60}
 domain={[0, 2]}
 range={[0, 2]}
/>

This is a function that grows quickly pretty fast. Let's say we had another
function that performs the same task as ${f,}$ but with a running time of

$$
 T(n) = n
$$

If we plot both functions:

<HPlot
 data={[
  { f: (x) => x ** 2, color: "var(--red)" },
  { f: (x) => x, color: "var(--blue)" },
 ]}
 scale={60}
 domain={[0, 2]}
 range={[0, 2]}
/>

Notice the intersection between these two plots:

$$
 (1,1)
$$

This tells us that at just before the point ${n = 1,}$ the original
polynomial algorithm grows slower than the linear algorithm. However, after
the point ${n = n,}$ the linear algorithm grows slower than the polynomial.
What does this mean in terms of running time? Well, it means that as the
inputs increase, the linear time algorithm takes less time than the
polynomial time algorithm. In other words, the linear time algorithm is
_faster_ than the polynomial time algorithm past ${n=1.}$

## Runtime Factors

The following factors impact runtime:

1. The algorithm

- As we've seen, mathematics dictates how fast or how slow a given
  algorithm runs.

2. The implementation details

- A poorly implemented algorithm can turn what we thought was a linear time
  algorithm to a quadratic time algorithm (e.g., a loop to iterate over a
  string, but mistakenly using within the loop some function that also
  iterates over the string to compute its length).

3. CPU processing speed

- All else equal, where ${n < m}$, the ${m}$-bit processor will execute the
  algorithm faster than the ${n}$-bit processor.

4. Memory read/write speeds

- A system with more caches will execute the algorithm faster than a system
  with fewer caches.

5. Compiler optimizations

- Setting a compiler's options to _debug mode_ will result in less
  optimized code to be executed at runtime.

6. Other programs running parallel

- The more programs we have running in the background, the less resources
  there are available.

7. Input data

- Larger inputs will take a longer time to process than smaller inputs.

## Input Size vs. Run Time

Of the factors listed previously, input size is independent of all the
other factors. Because of this fact, we can draw conclusions like the
following:

1. If we double the input size and the algorithm runs twice as slow (double
   the runtime), then the algorithm runs in linear time.
2. If we double the input size and the algorithm runs four times as slow,
   then the algorithm runs in quadratic time.
3. If we double the input size and the algorithm runs eight times as slow,
   then the algorithm runs in cubic time.

If want to be very accurate about assessing the input size factor, we have
to be clear about a few matters:

1. What counts as an input?

- Is the input an `int`? A `struct`? An instance of a class?

2. What is the word size of our system?

- Are we talking about a 64-bit processor? 32? 16?

If, however, we're just assessing the input size generally, we can just use
the following notation, provided every agrees that this is the notation to
use:

- ${n}$ is the input size.
- ${f(n)}$ is the number of steps required by the algorithm to execute,
  given an input size ${n.}$
- ${O(f(n))}$ is the _complexity class_ of ${f(n).}$

The most common complexity classes:

| Class Notation             | Class Name         |
| -------------------------- | ------------------ |
| ${O(1)}$                   | Constant           |
| ${O(\log n)}$              | Logarithmic        |
| ${O(n)}$                   | Linear             |
| ${O(n \log n)}$            | Linearithmic       |
| ${O(n^2)}$                 | Quadratic          |
| ${O(n^3)}$                 | Cubic              |
| ${O(n^x) : x \in \NN}$     | Polynomial         |
| ${O(C^n) : C \in \RR^{+}}$ | Exponential        |
| ${O(n!)}$                  | Factorial          |
| ${O(2^{2^n})}$             | Doubly exponential |

Graphing the most common run times:

<HPlot
 data={[
  { f: (n) => n, color: "teal" },
  { f: (n) => Math.log(n), color: "blue" },
  { f: (n) => n * Math.log(n), color: "purple" },
  { f: (n) => n ** 2, color: "green" },
  { f: (n) => 2 ** n, color: "orange" },
  { f: (n) => 1, color: "firebrick" },
 ]}
 domain={[0, 10]}
 range={[0, 25]}
 scale={50}
/>

Complexity analysis allows us to predict which algorithms will _eventually_
be faster. For example, algorithms that run on ${O(n^2)}$ time will take
longer than ${O(n)}$ algorithms. Why? Well, we can just look at their
graphs:

<HPlot
 data={[
  { f: (n) => n, color: "teal" },
  { f: (n) => n ** 2, color: "firebrick" },
 ]}
 domain={[0, 5]}
 range={[0, 10]}
 scale={50}
/>

Above, the teal line corresponds to an algorithm that runs at ${O(n).}$ The
red line corresponds to an algorithm that runs at ${O(n^2).}$ While the
${O(n^2)}$ algorithm runs faster initially, as the inputs increase, it will
take longer than the linear algorithm — the algorithm that runs at
${O(n).}$

Importantly, we drop constants when we conduct complexity analysis. Again,
we do so because they ultimately do not matter at very large inputs. For
example, consider the graphs of ${f(n) = 2n^2}$ (the teal line) and
${f(n) = 3n^2}$ (the red line):

<HPlot
 data={[
  { f: (n) => 2 *n ** 2, color: "teal" },
{ f: (n) => 3 * n** 2, color: "firebrick" },
 ]}
 domain={[0, 5]}
 range={[0, 5]}
 scale={50}
/>

Initially, it looks as if the algorithm running at ${f(n) = 3n^2}$ runs
faster than ${f(n) = 2n^2.}$ If, however, we passed very large inputs to
both functions, we see the following:

<HPlot
 data={[
  { f: (n) => 2 *n ** 2, color: "teal" },
{ f: (n) => 3 * n** 2, color: "firebrick" },
 ]}
 domain={[0, 500]}
 range={[0, 500]}
 scale={50}
/>

They're esentially indistinguishable.

## What Counts as a Step in an Algorithm?

Complexity analysis is supposed to give us a general idea of how many steps
an algorithm might take complete execution. Accordingly, a relevant
question is: What counts as a step?

We define a step as an operation that runs at ${f(1).}$ In any given
algorithm, these operations are:

1. Variable declaration
2. Variable assignment
3. Arithmetic operations
4. Comparison operations
5. Array indexing/pointer referencing
6. Calling a function
7. Returning from a function

In practice, these operations do not necessarily take exactly 1 step.
Addition, for example, is faster than multiplication, and multiplication is
faster than division. These details, however, are far from the high-level
source code we're working with. Accordingly, we will hand-waive and say
that the operations above all take constant time — ${O(1).}$

### Linear Step Counts

The for-loop is an operation that consists of several steps. Given ${n}$
inputs processed with basic steps:

1. __Initialization__ occurs once. This is ${1}$ step.
2. __Testing__ occurs each time the loop's body executes, as well as when
   the loop ends: ${n + 1}$ steps.
3. __Updating__ occurs every time the loop's body executes. Thus, this
   takes ${n}$ steps.

Hence, for the for-loop construct, given ${n}$ inputs and a loop body
consisting of basic steps, the loop runs at:

$$
\begin{aligned}
 f(n) &= 1 + n + 1 + n \\
 &= 2n + 2
\end{aligned}
$$

In big-O terms, ${f(n)}$ is of order ${O(n).}$

### Polynomial Step Counts

If we have a loop within a loop, then we _potentially_ run into polynomial
time. For example, suppose the nesting loop comprises two loops. The first
loop acts on ${n}$ inputs, and the second loop acts on ${n}$ inputs. In
that case, we have:

$$
\begin{aligned}
 f(n) &= 1 + n(1 + n + 1 + n) + 1 + 1 + n + 1 + n \\
 &= 1 + n(2 + 2n) + 3 + 2n \\
 &= 1 + 2n + 4n^2 + 3 + 2n \\
 &= 1 + 4n + 4n^2 + 3 \\
 &= 4n + 4n^2 + 4 \\
 &= 4(n + n^2 + 1) \\
 &= 4(n^2 + n + 1) \\
\end{aligned}
$$

Given that ${f(n) = 4(n^2 + n + 1),}$ we say that the construct has a
runtime of order ${O(n^2).}$ We emphazied the qualifier _potentially_
because a nested for-loop does not imply that a polynomial runtime. The
inner loop might only run ${C}$ times, where ${C}$ is a constant. In which
case:

$$
\begin{aligned}
 f(n) &= 1 + n(C) + (1 + n)(C) \\
 &= 1 + Cn + (C1 + Cn) \\
 &= 1 + C(2n + 1) \\
\end{aligned}
$$

Dropping the constants (1, 2, and ${C}$), we get a runtime function of
order ${O(n).}$ Such loops aren't all that rare. We see them in loops with
headers like:

```c
for (int j = 0; j < 5; j++) {
 // basic steps
}
```

This loop only runs 5 times, which is a constant. Similarly, a nested
for-loop does not imply runtime function of order ${O(n^2).}$ If we had a
for-loop like:

```c
for (int i = 0; i < getWordCount(wordArray); i++) {
 // basic steps
 for (int j = 0; j < getCharacterCount(wordArray[i]); j++) {
 // basic steps
 }
}
```

and both the `getWordCount` and `getCharacterCount` functions compute their
outputs by _also_ using their own for-loops, we now have an algorithm that
possibly runs at ${f(n) = n(n(n(n))).}$ I.e., an algorithm with a run time
function of order ${O(n^4).}$

### Logarithmic Step Counts

Consider the following for-loop:

```c
int f(int n) {
  int sum = 0;
  for (int i = n; i > 1; i /= 2) {
    sum += i;
  }
  return sum;
}
```

Here, the variable `i` starts at `n`, and is divided by two each time.
Thus, the function `f` has a runtime function of
${f(n) = 1 + 1 + \log_{2} n.}$ This is a runtime function of order
${O(\log n).}$

### For-loops Generally

Because of how common loops are, it's generally a good idea to build some
gut-instincts whenever we see loop headers:

<div className="lay3">

<article>

<section>

```c
for (i = 0; i < n; i++)
```

</section>

<section>

As is, this loop executes ${n}$ times.

</section>

</article>

<article>

<section>

```c
for (i = n; i > 0; i--)
```

</section>

<section>

This loop is decrementing. Nevertheless, it still executes ${n}$ times.

</section>

</article>

<article>

<section>

```c
for (i = 0; i < n; i+=2)
```

</section>

<section>

This loop increments by two each time. So, it executes:

$$
 \dfrac{n}{2}
$$

times.

</section>

</article>

<article>

<section>

```c
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++)
```

</section>

<section>

This loop executes ${n}$ times, then for each ${n,}$ another ${n}$ times.
Result:

$$
 n^2
$$

</section>

</article>

<article>

<section>

```c
for (i = 0; i < n; i++)
  for (j = 0; j < i; j++)
```

</section>

<section>

This loop executes:

$$
 \dfrac{n(n+1)}{2}
$$

To see why this is the case, consider the following table:

| ${i}$ | ${j}$            | loops |
| ----- | ---------------- | ----- |
| 0     | 0                | 0     |
| 1     | 0 ${\checkmark}$ | 1     |
|       | 1 ${\times}$     |       |
| 2     | 0 ${\checkmark}$ | 2     |
|       | 1 ${\checkmark}$ |       |
|       | 2 ${\times}$     |       |
| 3     | 0 ${\checkmark}$ | 3     |
|       | 1 ${\checkmark}$ |       |
|       | 2 ${\checkmark}$ |       |
|       | 3 ${\times}$     |       |

Accordingly, we have:

$$
 1 + 2 + 3 + \ldots + n
$$

This corresponds to the series:

$$
 \dfrac{n(n+1)}{2}
$$

</section>

</article>

<article>

<section>

```c
p = 0

for (i = 1; p <= n; i++)
  p = p + i
```

</section>

<section>

Here, the loop invariant is still ${n,}$ but the variable we're testing
against is ${p.}$ Moreover, ${p}$'s value is depending on ${i.}$ This
yields the following trace:

| ${i}$      | ${p}$                |
| ---------- | -------------------- |
| 1          | ${0 + 1 = 1  }$      |
| 2          | ${1 + 2 = 3  }$      |
| 3          | ${3 + 3 = 6  }$      |
| 4          | ${6 + 4 = 10 }$      |
| 5          | ${10 + 5 = 15}$      |
| ${\vdots}$ | ${\vdots}$           |
| ${k}$      | ${1+2+3+4+\ldots+k}$ |

What this tells us is that the loop will step when:

$$
 \dfrac{k(k+1)}{2} \gtn n
$$

Asymptotically, this comes out to roughly:

$$
 T(n) \approx \sqrt{n}
$$

</section>

</article>

<article>

<section>

```c
for (i = 1; i < n; i = i * 2)
```

</section>

<section>

This loop runs from ${i = 0}$ to ${n,}$ and at the end of each iteration,
updates ${i}$ by multiplying its previous value by ${2.}$ Tracing:

| loop       | old ${i}$  | new ${i}$                 |
| ---------- | ---------- | ------------------------- |
| 1          | 1          | ${1 \times 2 = 2 = 2^1}$  |
| 2          | 2          | ${2 \times 2 = 4 = 2^2}$  |
| 3          | 4          | ${4 \times 2 = 8 = 2^3}$  |
| 4          | 8          | ${8 \times 2 = 16 = 2^4}$ |
| ${\vdots}$ | ${\vdots}$ | ${\vdots}$                |
| ${\lg n}$  |            | ${2^{k}}$                 |

Because we're multiplying by ${i}$ by ${2}$ at each looop, we're heading
towards ${n}$ much faster. Specifically:

$$
 \T(n) = \lg n
$$

This leads to a more general conclusion about these types of loops. Given a
loop of the form:

```rust
for (let i = 1; i < n; i = i * 𝐶)
```

where `𝐶` is some integer greater than ${1}$, without more, the loop runs:

$$
 \T(n) \approx \ceil{\log_{C} n}
$$

times. Notice that we use the ceiling function for the runtime function's
definition. We do so because the loop runs from ${1}$ to ${n.}$ If ${n}$ is
not a power of `𝐶`, we can end at some floating point value. For example,
if ${n = 8,}$ we get:

$$
 \lg 8 = 3
$$

However, if ${n = 10}$ (not a power of ${2}$), we get:

$$
 \lg 10 \approx 3.322
$$

We must take the floor of this result. To confirm, compare the traces:

| Loops | ${i \ltn 8}$ | ${i \ltn 10}$ |
| ----- | ------------ | ------------- |
| 1     | 1            | 1             |
| 2     | 2            | 2             |
| 3     | 4            | 4             |
| 4     | 8 ${\times}$ | 8             |
|       |              | 16 ${\times}$ |

</section>

</article>

<article>

<section>

```rust
for (i = n; i >= 1; i = i / 2)
```

</section>

<section>

This loop divides by the variant by ${2}$ each time. Once again, it's easy
to see what's happening with this loop if we just traced:

| loop       | ${i}$                       | ${i}$              |
| ---------- | --------------------------- | ------------------ |
| 1          | ${(n/2)}$                   | ${n/2}$            |
| 2          | ${(n/2)/2}$                 | ${n/4}$            |
| 3          | ${((n/2)/2)/2}$             | ${n/8}$            |
| 4          | ${(((n/2)/2)/2)/2}$         | ${n/16}$           |
| 5          | ${((((n/2)/2)/2)/2)/2}$     | ${n/32}$           |
| 6          | ${(((((n/2)/2)/2)/2)/2)/2}$ | ${n/64}$           |
| ${\vdots}$ | ${\vdots}$                  | ${\vdots}$         |
| ${\lg n}$  | ${\vdots}$                  | ${\dfrac{n}{2^k}}$ |

</section>

</article>

<article>

<section>

```rust
for (i = 0; i * i < n; i++)
```

</section>

<section>

This loop runs on ${\T(n) \approx \sqrt{n}.}$ Easy to check:

$$
 \begin{aligned}
 i^2 &\ltn n \\[1em]
 i &\ltn \sqrt{n}
 \end{aligned}
$$

</section>

</article>

<article>

<section>

```rust
for (i = 0; i < n; i++)

for (j = 0; j < n; j++)
```

</section>

<section>

These loops are not tested. So, we've got a simple sum:

$$
 n + n = 2n
$$

</section>

</article>

<article>

<section>

```rust
p = 0

for (i = 1; i < n; i = i * 2) {
 p++
}

for (j = 1; j < p; j = j * 2)
```

</section>

<section>

Given that these loops aren't nested, we're looking at another sum. For the
second loop, we have:

$$
 \lg p
$$

Now, before that ${p}$ gets to the second loop, it's mutated by the first
loop. There, its modified ${\lg n}$ times. It follows that we have:

$$
 p = \lg n
$$

So, when it gets to the second loop, we have a runtime function of:

$$
 \T(n) = \lg p = \lg(\lg n)
$$

</section>

</article>

<article>

<section>

```rust
for (i = 0; i < n; i++)
   for (j = 1; j < n; j = j * 2)
```

</section>

<section>

Here, the outer loop increments linearly. The inner loop, however, increments logarithmically. Thus, we have:

$$
  \T(n) = n \cdot \lg n
$$

</section>

</article>

</div>

### Slight Implementation Changes, Drastic Complexity Changes

Say we were given the following code:

```c
void f(int *out, int *in, int size) {
   for (int i = 0; i < size; ++i) {
   out[i] = 1;
      for (int j = 0; j < size; ++j) {
         if (i != j) out[i] *= in[j];
      }
   }
}
```

This function modifies the `out` array by multiplying each element of `out`
with the sequence product of `in`. I.e.,

```nasm
out[i] = in[0] * in[1] * ... * in[i-1] * in[i+1] * ... * in[size-1];
```

Here, we see a condition in the inner-loop, `i != j`. It's likely that most
of the time, this condition is true. In which case, the upper bound of the
runtime function is ${f(n) = n^2.}$ However, if we wanted to be more exact,
we can draw from the fact that `i == j` at least once. In which case we
have ${f(n) = n^2 - n.}$ Either or, the runtime function will be of order
${O(n^2).}$

Now consider a different version of the code:

```c
void f(int *out, int *in, int size) {
   int product = 1;
   for (int i = 0; i < size; ++i) {
      product *= in[i];
   }
   for (int i = 0; i < size; ++i) {
      out[i] = product / in[i];
   }
}
```

Here, we first compute the sequence product of `in`. Then, for each element
of `out`, we assign the sequence product divided by the corresponding
element of `in`. Examining this implementation, we see that `f` now has the
runtime function ${f(n) = 2n.}$ This is a runtime function of order
${O(n).}$

As is, this algorithm is _input sensitive_ — it will break for certain
inputs. Obviously it will break for very large `int`s, but will also break
if any `in[i]` turns out to be zero. Fortunately, there's a fix:

```c
void f(int *out, int *in, int size) {
 int product = 1;
 for (int i = 0; i < size; ++i) {
 if (in[i] == 0) goto ZEROCASE;
 product *= in[i];
 }
 for (int i = 0; i < size; ++i) {
 out[i] = product / in[i];
 }
 ZEROCASE:
 for (int i = 0; i < size; ++i) {
 out[i] = 0;
 }
}
```

## Log and Power Rules

Becuase of how common logarithmic and polynomial times are, below is a
review of log and power identities.

| Identity                                                      | Example                                       |
| ------------------------------------------------------------- | --------------------------------------------- |
| ${\log_{a}(xy) = \log_{a}x + \log_{a}y}$                      | ${\log_{2}(12) = \log_{2}(6) + \log_{2}(2)}$  |
| ${\log_{a}(x/y) = \log_{a}(x) - \log_{a}(y)}$                 | ${\log_{2}(4/3) = \log_{2}(4) - \log_{2}(3)}$ |
| ${\log_{a}(x^r) = r \log_{a}x}$                               | ${\log_{2}8 = \log_{2}(2^3) = 3 \log_{2}(2)}$ |
| ${\log_{a}x = \dfrac{\log x}{\log a} = \dfrac{\ln x}{\ln a}}$ | ${\log_{7}9 = \dfrac{\ln 7}{\ln 9}}$          |
| ${\log_{a}(a) = 1}$                                           | ${\log_{2}(2) = 1}$                           |
| ${\log_{a}(1) = 0}$                                           | ${\log_{2}(1) = 0}$                           |
| ${a^n a^m = a^{n+m}}$                                         | ${2^{5} = 2^{2} 2^{3}}$                       |
| ${\dfrac{a^n}{a^m} = a^{n-m}}$                                | ${\dfrac{2^3}{2^2} = 2^{3-2} = 2^{1}}$        |
| ${a^{xy} = (a^{x})^{y}}$                                      | ${2^{16} = (2^{4})^{4}}$                      |
| ${\dfrac{1}{a^n} = a^{-n}}$                                   | ${2^{-2} = \dfrac{1}{2^2}}$                   |
| ${a^{-1} = \dfrac{1}{a}}$                                     | ${2^{-1} = \dfrac{1}{2}}$                     |
| ${a^1 = a}$                                                   | ${2^1 = 2}$                                   |
| ${a^0 = 1}$                                                   | ${2^0 = 1}$                                   |

Remember that equality can always be read both ways!

<Grid cols={2} >

| Log Identities                                    |
| ------------------------------------------------- |
| ${\log_{a}(xy) = \log_{a}x + \log_{a}y}$          |
| ${\log_{a}x + \log_{a}y = \log_{a}(xy)}$          |
| ${\log_{a}(x/y) = \log_{a}(x) - \log_{a}(y)}$     |
| ${\log_{a}(x) - \log_{a}(y) = \log_{a}(x/y)}$     |
| ${\log_{a}(x^r) = r \log_{a}x}$                   |
| ${r \log_{a}x = \log_{a}(x^r)}$                   |
| ${\log_{a}x = \dfrac{\ln x}{\ln a}}$              |
| ${\log_{a}x = \dfrac{\log x}{\log a}}$            |
| ${\dfrac{\log x}{\log a} = \dfrac{\ln x}{\ln a}}$ |
| ${\dfrac{\ln x}{\ln a} = \dfrac{\log x}{\log a}}$ |
| ${\dfrac{\ln x}{\ln a} = \log_{a}x}$              |
| ${\log_{a}(a) = 1}$                               |
| ${1 = \log_{a}(a)}$                               |
| ${\log_{a}(1) = 0}$                               |
| ${0 = \log_{a}(1)}$                               |

| Power Identities               |
| ------------------------------ |
| ${a^n a^m = a^{n+m}}$          |
| ${a^{n+m} = a^n a^m }$         |
| ${\dfrac{a^n}{a^m} = a^{n-m}}$ |
| ${a^{n-m} = \dfrac{a^n}{a^m}}$ |
| ${a^{xy} = (a^{x})^{y}}$       |
| ${(a^{x})^{y} = a^{xy}}$       |
| ${\dfrac{1}{a^n} = a^{-n}}$    |
| ${a^{-n} = \dfrac{1}{a^n}}$    |
| ${a^{-1} = \dfrac{1}{a}}$      |
| ${\dfrac{1}{a} = a^{-1}}$      |
| ${a^1 = a}$                    |
| ${a = a^1}$                    |
| ${a^0 = 1}$                    |
| ${1 = a^0}$                    |

</Grid>

Note that some texts will use special notation for certain logarithms:

| Notation    | Description                                                                  |
| ----------- | ---------------------------------------------------------------------------- |
| ${\lg (n)}$ | Equivalent to ${\log_{2}n.}$ Commonly found in older computer science texts. |
| ${\log(n)}$ | In mathematics, assumed to be ${\log_{10}n.}$                                |
| ${\ln(n)}$  | The natural log, ${\log_{e}(n).}$                                            |

## Summary of Landau Notation

Notice some of the tradeoffs with each of these notations. With Big-O, we
can make _too many_ claims. ${f(n) = n}$ is a member of ${O(n),}$ but it's
also a member of ${O(n^2),}$ ${O(n^3),}$ and so on. Analogizing, it's akin
to a baseball scout asking, "What's the upper bound on his pitch velocity?"
and us answering, "Well, it can't get any faster than 300 miles per hour."
But, we could have also said, 400 miles per hour, 500 miles per hour, or
even the speed of light — all of which would satisfy Big-O.

We face a similar problem with Big-Omega, only the other way around. Given
${f(n) = n^3}$ there's a lower bound of ${\Omega(n^2),}$ but there's also a
lower bound of ${\Omega(n),}$ ${\Omega(\lg n),}$ and ${\Omega(1).}$ If our
pitcher pitched at 100 miles per hour, we can say that he pitches no slower
than than 90 miles per hour, but we can also say that he pitches no slower
than 0 miles per hour.

That said, Big-O is often used in practice for several reasons: (1) It's
easy to carry out analytically. For many algorithms, the most meticulous
Big-O analysis requires a tiny bit of calculus, if that. (2) Many of the
most common computation problems have clear worst-case analyses. And (3)
Big-O is so widely-understood that it's the most efficient form of
communicating bounds.

That said, whenever we can prove Big-Theta, we should opt for the tight
bound. Big-Theta has the advantage of communicating a single, clear
boundary.

| Function                | Big-O                                    | Big-Theta        | Big-Omega                                     |
| ----------------------- | ---------------------------------------- | ---------------- | --------------------------------------------- |
| ${f(n) = 2n + 1}$       | ${O(n),}$ ${O(n^2),}$ ${O(n^3), \ldots}$ | ${\Theta(n)}$    | ${\Omega(n)}$ or ${\Omega(1)}$                |
| ${f(n) = 2n^2 + n + 5}$ | ${O(n^2),}$ ${O(n^3), \ldots}$           | ${\Theta (n^2)}$ | ${\Omega(n^2),}$ ${\Omega(n),}$ ${\Omega(1)}$ |

## Amortized Complexity

__Amortized complexity__ is a kind of worst-case analysis. It's the
analysis used when the algorithm's work or time profile is "spiky." That
is, for certain inputs, the algorithm is very expensive, and for others,
it's a tiny expense.

The classic example of such an algorithm is the `extend` function of the
_vector ADT_. Whenever we push a new element into the vector, there are two
possible outcomes: Either there's enough space, or there isn't enough
space. If there is enough space, pushing a new element runs in constant
time — we just index into the available space and assign the element there.
If, however, there isn't enough space, then we must extend before
assigning: the vector must create a new static array with additional
spaces, then iterate through all of the existing elements, copying each
element to the new array. This takes a linear amount of time. Thus, we have
a situation where, sometimes the push algorithm takes a constant amount of
time, and sometimes it takes a linear amount of time.

To perform this complexity analysis, we consider the average cost of _one_
operation over a _sequence of operations_.
